\documentclass[11pt, sakura, night, 0.5in]{LatexTemplate/hw}

\def\course{MAT240: Algebra I}
\def\headername{Definition Sheet}
\def\name{Joseph Siu}
\def\info{Most of the numbers are correct. This sheet also does not include the examples.}
\def\logo{\clsfiles/qunwang}

\useclspackage{mat240}


\begin{document} 

\coverpage

\newch[Fields and Polynomials]{ch1}{
    
    \newd[Field]{def1}{
        A field $\F$ is a set with two special elements "$0\in \F$" and "$1\in \F$" and two operations "$+$" and "$\cdot$" which satisfy the following axioms.
        \begin{enumerate}
            \item (Commutativity) For all $x,y\in\F$ we have: $x+y=y+x$ and $x\cdot y=y\cdot x$.
            \item (Associativity) For all $x,y,z\in\F$ we have: $(x+y)+z=x+(y+z)$ and $(x\cdot y)\cdot z=x\cdot(y\cdot z)$.
            \item (Distributivity) For all $x,y,z\in\F$ we have: $x\cdot(y+z)=x\cdot y+x\cdot z$.
            \item (Identities) For all $x\in\F$ we have: $x+0=x$ and $x\cdot 1=x$.
            \item (Inverses) For all $x\in\F$ there exists $y\in\F$ such that $x+y=0$. For all $x\in\F\setminus\{0\}$ there exists $z\in\F$ such that $x\cdot z=1$.
        \end{enumerate}
        \newn{
            $\Q$ is a field where $\Z$ is not a field becasue of the absence of some multiplicative inverse.
        }
    }
    \newco[Field]{coro1}{
        Let $\F$ be a field and $a,b,c\in\F$.

        \begin{enumerate}
            \item If $a+c=b+c$, then $a=b$.
            \item If $c\neq 0$ and $c\cd a = c\cd b$, then $a=b$.
            \item The field elements $0,1$ are unique.
            \item The elements $y$ and $z$ from Axoim $5$ are unique. (From now on, we will denote the additive inverse of $x$ by $-x$, and the multiplicative inverse of $x$ by $x^{-1}.$)
            \item $a\cd 0=0$.
            \item $(-a)\cd(b)=-(a\cd b)=(a)\cd(-b).$
            \item $-(-a)=a$. If $a\neq0$, then $\bra{a^{-1}}^{-1}=a$.
            \item If $a\cd b=0$, then $a=0$ or $b=0$.
        \end{enumerate}
    }
        
    \envbreak
    \newt[Equivalence]{thm1}{
        \begin{enumerate}
            \item $\sim$ is an equivalence relation.
            \item $a\sim b$ if and only if $a$ and $b$ have the same remainder when divided by $n$.
            \item There are exactly $n$ equivalence classes for this relation: $[0],[1],\ldots,[n-1]$ - one for each possible remainder for division by $n$.
        \end{enumerate}
    }

    \newd[$\Z_n$]{def2}{
        Let $\Z_n=\{[0],[1],\ldots,[n-1]\}$ be the est of equivalence classes for this equivalence relation. We define $+,\cd$ on $\Z_n$ as follows:
        \[[a]+[b]=[a+b],\]
        \[[a]\cd[b]=[a\cd b].\]
    }

    \envbreak
    \newt[Quadratic Formula]{thm2}{
        Let $a,b,c\in\R$. The quadratic equation $ax^2+bx+c=0$ (where $a\neq0$) has:
        \begin{enumerate}
            \item Solutions $x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}$ if $b^2-4ac\geq0.$
            \item No solutions if $b^2-4ac<0.$
        \end{enumerate}
    }

    \newd[Complex]{def3}{
        Let $i=\sqrt{-1}$. I.e. $i$ is a number with the property that $i^2=-1$.

        Let $\C=\{a+bi \mid a,b\in\R\}.$ We call $\C$ the set of complex numbers and we define addition and multiplication $+,\cd$ in the obvious ways:
        \[(x+yi)+(a+bi)=(x+a)+(y+b)i\]
        \[(x+yi)\cd(a+bi)=ax+bi+ayi+byi^2=(ax-by)+(ay+bx)i\]
        \newn{
            $\C$ is a field.
        }

        Given a complex number $z=x+yi$, we define its \underline{\textbf{conjugate}} by: \[\overline{z}=x-yi.\]

        We define the \underline{\textbf{length (or modulus)}} of a complex number by: \[|z|=\sqrt{x^2+y^2}.\] 

        Note that we in the $xy$-plane, we obtain $\overline{z}$, the conjugate of $z$, by reflecting $z$ in the $x$-axis, and the length of a complex number is just the usual distance from $z$ to the origin in the $xy$-plane.
    }

    \newt[Complex]{thm3}{
        For any $z,w\in\C$ we have:
        \begin{enumerate}
            \item $\overline{z+w}=\overline{z}+\overline{w}$.
            \item $\overline{z\cd w}=\overline{z}\cd\overline{w}$.
            \item $\overline{\frac{z}{w}}=\frac{\overline{z}}{\overline{w}}$ (provided $w\neq0$).
            \item $\overline{\overline{z}}=z$.
            \item $z\overline{z}=|z|^2$.
            \item $z^{-1}=\frac{\overline{z}}{|z|^2}$ (provided $z\neq0$).
            \item $|zw|=|z||w|$.
            \item $\abs{\frac{z}{w}}=\frac{|z|}{|w|}$ (provided $w\neq0$).
            \item $|z+w|\leq |z| + |w|$ ("Triangle nequality for Complex Numbers").
        \end{enumerate}
    }

    \envbreak
    \newd[Polar Form]{def4}{
        For $z=x+yi$, we define its \underline{\textbf{polar form}} as $z=re^{i\theta}$, where $r=|z|=\sqrt{x^2+y^2}$ and $\theta$ is the angle between $z$ and the positive $x$ axis (measured counterclockwise, in radians). The angle $\theta$ is called the \underline{\textbf{argument}} of $z$, and $r$ is called the \underline{\textbf{length (or modulus)}} of $z$.
    }
    \newt[Polar Form]{thm4}{
        Let $z=re^{i\theta}$, $w=Re^{i\phi}$.
        \begin{align*}
            zw=rRe^{i(\theta+\phi)}\\
            z^n=r^ne^{in\theta}\\
        \end{align*}
    }

    \envbreak
    \newd[Polynomial]{def5}{
        A polynomial $p$ with coefficient from $\F$ is an expression \[p(x)=c_0+c_1 x + c_2 x^2 +\cdots + c_n x^n\] where $c_i\in\F$. We call the field elements $c_0,\ldots,c_n$ the "coefficients" of $p$.

        The largest exponent $n$ so that $c_n\neq0$ is called the \tbf{degree} of $p$, and we typically write $\deg p=n$. Constant polynomials are degree 0.

        The set of all polynomials over $\F$ is denoted by $P(\F)$.

        The set of all polynomials of degree \tbf{less than or equal} to $n$ is denoted by $P_n(\F)$.
    }
    \newt[Polynomial]{thm5}{
        Let $\F$ be a field, and $f,g\in P(\F)$ be non-zero polynomials. Then there exist unique polynomials $q,r\in P(\F)$ so that:
        \begin{enumerate}
            \item $f(x)=q(x)g(x)+r(x)$.
            \item $\deg r<\deg g$ if $\deg g\neq0$.
            \item $r=0$ if $\deg g=0$.
        \end{enumerate}
    }
    \newd[Polynomial Cont.]{def6}{
        Let $\F$ be a field and $f,g\in P(\F)$. We say that $g$ divides $f$ if $f(x)=q(x)g(x)$ for some polynomial $q\in P(\F)$.

        We say that a non-constant polynomial $p\in P(\F)$ is "irreducible" if we \tbf{cannot} express $p$ as a product of polynomials of smaller degree.

        I.e. $p$ is irreducible if we \tbf{cannot} write $p(x)=g(x)q(x)$ for any polynomials $g,q\in P(\F)$ with the property that both $\deg g, \deg q < \deg p$. 

        \newn{
            $f(x)=x^2-2$ is irreducible over $\Q$ but not over $\R$.
        }
    }

    \newt[Polynomial Cont.]{thm6}{
        Let $\F$ be a field, $p\in P(\F)$ and $\deg p\geq 1$. Then $a\in \F$ is a root of $p$ if and only if $x-a $ divides $p$. 
    }
    \envbreak

    \newt[Fundamental Theorem of Algebra]{thm7}{
        Every non-constant polynomial has a root over $\C$.

        In fact, every non-constant polynomial factors completely into a product of linear terms over $\C$.
    }
}

\np
\tcbcnt{definition}{8}
\tcbcnt{theorem}{10}
\newch[Linear Systems]{ch2}{
    \newd[Linear]{d9}{
        Let $\F$ be a field and $b,c_1,\ldots,c_n\in\F$. An equation in the variables $x_1,\ldots,x_n$ is called \tbf{linear} if it can be expressed as $c_1x_1+c_2x_2+\cdots+ c_n x_n=b$.
    }

    \newd[System of Equations]{d10}{
        Let $\F$ be a field, and $a_{ij}\in\F$ (where $i\in\{1,\ldots,m\}$ and $j\in\{1,\ldots, n\}$). A \tbf{system of linear equations} in variables $x_1,x_2,\ldots,x_n$ is a finite collection of linear equations in $x_1,x_2,\ldots,x_n$: 
        
        \[\begin{aligned}
            a_{11}x_{1}+a_{12}x_{2}+\cdots+a_{1n}x_{n}& =b_{1}  \\
            a_{21}x_{1}+a_{22}x_{2}+\cdots+a_{2n}x_{n}& =b_{2}  \\
            \vdots&\\\
            a_{m1}x_{1}+a_{m2}x_{2}+\cdots+a_{mn}x_{n}& =b_{m}.     
        \end{aligned}\] 
        
        A system of $m$ equations with $n$ unkowns is called an \tbf{$m\times n$ system}.
    }

    \newd[Solutions]{d11}{
        A \tbf{solution} to a linear equation $c_1x_1+c_2x_2+\cdots + c_n x_n=b$ is a choice of field elements $s_1,s_2,\ldots,s_n\in\F$, so that when we substitute them for $x_1,x_2,\ldots,x_n$ respectively, the resulting equation is true.

        That is, we have $c_1s_1+c_2s_2+\cdots+c_n s_n=b$ (i.e. the left- and right-hand sides are equal.)

        A \tbf{solution to a system} is a choice of field elements $s_1,s_2,\ldots,s_n$ which solves $\textit{every}$ equation of the system.
    }

    \newd[Consistent]{d12}{
        If a system of equations has at least one solution, we say it is \tbf{consistent}.

        If a system of equations has no solutions, we say it is \tbf{inconsistent}.
    }
    \envbreak

    \newd[Matrix]{d13}{
        An \tbf{$m\times n$ matrix} over $\F$ is a rectangular array of field elements consisting of $m$ rows and $n$ columns.

        We denote the $j^{\T{th}}$ entry in row $i$ of matrix $A$, by \tbf{$a_{ij}$}, and call it the \tbf{$ij^{\T{th}}$ entry} of $A$.
    }

    \newd[Augmented Matrix]{d14}{
        Consider a system of equations: \[
            \begin{aligned}
                a_{11}x_{1}+a_{12}x_{2}&+\cdots+a_{1n}x_{n}& =b_{1}  \\
                a_{21}x_{1}+a_{22}x_{2}&+\cdots+a_{2n}x_{n}& =b_{2}  \\
                &\vdots &\vdots\\
                a_{m1}x_{1}+a_{m2}x_{2}&+\cdots+a_{mn}x_{n}& =b_{m} 
            \end{aligned}  
        \]

        We defie the \tbf{augmented matrix} \textit{corresponding to} the system of equations above to be: \[
            \left.\left(\begin{array}{cccc|c}a_{11}&a_{12}&\cdots&a_{1n}&b_{1}\\a_{21}&a_{22}&\cdots&a_{2n}&b_{2}\\&&\cdots&&\\a_{m1}&a_{m2}&\cdots&a_{mn}&b_{m}\end{array}\right.\right),
            \]
    }
    \newd[RREF]{d15}{
        We say a matrix $A$ is in \underline{\textbf{reduced row echelon form}} if \textit{all} of the following conditions are met:
        \begin{enumerate}
            \item All zero rows are at the bottom of the matrix $A$.
            \item The first non-zero entry in each non-zero row is a $1$. (Such entries are called ``leading 1's''.)
            \item The leading 1's move to the right, as we go down the rows of $A$.
            \item All entries above and below a leading 1 are 0.
        \end{enumerate}

        We will use the abbreviation "RREF" for "row-reduced echelon form", for the rest of the text.

        \newn{
            All matrices have a unique RREF.
        }
    }

    \newt[Gaussian Elimination]{thm11}{
        To "row reduce" a matrix perform the following steps:
        \begin{enumerate}
            \item If the matrix consists entire of 0's, stop. It's already row-reduced.
            \item Find the first column with a non-zero entry and move the corresponding row to the top. (We will call the first non-zero entry $a$.)
            \item Divide the row by the number $a$ to obtain a leading one.
            \item Subtract multiples of ths row from the rows above and below, in order to make each entry above and below the leading 1 equal to 0.
            \item Repeat 1-4 on the matrix consisting of the remaining rows.
        \end{enumerate}
    }

    \newd[Variables]{def16}{
        Suppose that $R$ is a matrix in RREF. We say that $x_i$ is a \underline{\tbf{leading variable}} if column $i$ contains a leading one. If a variable is not "leading" we call it a \underline{\tbf{non-leading variable}}.
    }
    \newr{
        To solve a system:
        \begin{enumerate}
            \item Row reduce the augmented coefficient matrix.
            \item If there is a row of the form $(\quad0\quad0\quad\cdots\quad0\quad|\quad1\quad)$ then there are no solutions.
            \item Otherwise, assign the non-leading variables (if any) parameters, and use the equatoins coming from the rows of the RREF to solve for each variable interms of the parameters.
        \end{enumerate}
    }

    \newd[Homogeneous]{d17}{
        A system of equations is called \underline{\tbf{homogeneous}} if it is of the form \[\begin{aligned}
            a_{11}x_{1}+a_{12}x_{2}+\cdots+a_{1n}x_{n}& =0  \\
            a_{21}x_{1}+a_{22}x_{2}+\cdots+a_{2n}x_{n}& =0  \\
            &\vdots \\
            a_{m1}x_{1}+a_{m2}x_{2}+\cdots+a_{mn}x_{n}& =0. 
            \end{aligned}\]
        In other words, it is homogeneous if the constant term (or right hand side) of \tbf{\textit{each}} equation in the system is 0.
        \newn{
            \begin{itemize}
                \item $x_1=0,x_2=0,\ldots,x_n=0$ is \tbf{\textit{always}} a solution to any homogeneous equation.
                \item We call this solution the \tbf{\textit{trivial}} solution.
                \item Any other solution is called a \tbf{\textit{non-trivial}} solution.
            \end{itemize}
        }
    }
}

\np
\newch[Vector Spaces]{ch3}{
    \newd[Vector Operators]{d18}{
        Given two vectors \tbf{v,w} we define \underline{\tbf{their sum}} \tbf{v+w} using “tip to tail" addition (or the “parallelogram law of addition"). In the diagram in the margin, the vector \tbf{v+w} is diagonal in the parallelogram spanned by v, w that shares its tail with v and w.

        We can also define their \tbf{\underline{difference}} \tbf{v-w} geometrically using the same parallelogram: v- w is the diagonal going from the tip of w to the tip of v.

        Finally, given a vector v and real number $a\in\mathbb{R}$, we can \underline{\tbf{scale v by $a$}} as follows:
        
        \begin{itemize}
            \item $0\mathbf{v}=\mathbf{0}.$
            \item If $a>0$, then $a\mathbf{v}$ is a vector pointing in the same direction as v with length scaled by $a$
            \item If $\alpha<0$, then $\alpha$ is a vector pointing in the opposite direction as v with length scaled by $|\alpha|.$
        \end{itemize}

        \newn{
            If $v=(x,y,z)$ and $w=(p,q,r)$, then $v+w=(x+p,y+q,z+r)$, $av=(ax,ay,az)$.
        }
    }

    \tcbcnt{definition}{19}

    \newd[$\F^n$]{def20}{
        Let $\F$ be a field. Consider the set $\F^n=\{(x_1,x_2,\ldots,x_n)\mid x_1,x_2,\ldots,x_n\in\F\}$. We can define two operations on $\F^n$ which we call "vector addition" which is a map $\F^n\times\F^n\ra\F^n$, and "scaling" which is a map $\F\times\F^n\ra\F^n$ as follows.

        For $v=(x_1,x_2,\ldots,x_n), w=(y_1,y_2,\ldots,y_n)\in\F^n$, and $c\in\F$ we define:
        \begin{align*}
            \Aleq{v+w}{(x_1,x_2,\ldots, x_n) + (y_1,y_2,\ldots,y_n)}{(vector addition)}
            &=(x_1+y_1,x_2+y_2,\ldots,x_n+y_n) &\\
            \Aleq{cv}{c(x_1,x_2,\ldots,x_n)}{(scaling)}
            &=(cx_1,cx_2,\ldots,cx_n) &
        \end{align*}
    }

    \tcbcnt{theorem}{12}
    \newt[$\F^n$]{thm13}{
        Let $\F$ be a field. Set $0=(0,0,\ldots,0)$. For any $v,w,u\in\F^n$ and $a,b\in\F$ we have:
        \begin{enumerate}
            \item $v+w=w+v$.
            \item $v+(w+u)=(v+w)+u$.
            \item $a(v+w)=av+aw$.
            \item $(a+b)v=av+bv$.
            \item $(ab)v=a(bv).$
            \item $1v=v$.
            \item $0+v=v$.
            \item For every $v\in V$ there exists $w\in V$ so that $v+w=0$.
        \end{enumerate} 
    }

    \envbreak
    \newd[Vector Space]{def21}{
        Let $\F$ be a field. A vector space $V$ over $\F$ is a non-empty set, containing a special element 0, with two operatoins $V\times V\ra V$ (called vector addition) and $\F\times V\ra V$ (called scaling) so that for all $v,w,u\in V$ and $a,b\in\F$:
        \begin{enumerate}
            \item $v+w=w+v$.
            \item $v+(w+u)=(v+w)+u$.
            \item $a(v+w)=av+aw$.
            \item $(a+b)v=av+bv$.
            \item $(ab)v=a(bv).$
            \item $1v=v$.
            \item $0+v=v$.
            \item For every $v\in V$ there exists $w\in V$ so that $v+w=0$.
        \end{enumerate}
        \newn{
            $P(\F), P_n(\F)$ and $\F^n$ are vector spaces.
        }
    }

    \envbreak
    \newd[Matrix Cont.]{def22}{
        Let $\F$ be a field. An $m\times n $ \tbf{matrix $M$ with entries in $\F$} is a rectangular array of elements of $\F$ consisting of $m$ rows and $n$ columns.

        We denote the entry in the $i$ row and $j$ column of a matrix $m$ by $m_{ij}$.

        The set of all $m\times n$ matrices with coefficients in $\F$ is denoted by $\mathcal{M}_{m\times n}(\F)$.

        For example, a $2\times3$ matrix looks like $\begin{pmatrix}m_{11}&m_{12}&m_{13}\\m_{21}&m_{22}&m_{23}\end{pmatrix}\in\mathcal{M}_{2\times3}(\F)$, wile a $3\times 2$ matrix $N$ looks like $\begin{pmatrix}n_{11}&n_{12}\\n_{21}&n_{22}\\n_{31}&n_{32}\end{pmatrix}\in\mathcal{M}_{3\times2}(\F)$.

        \newn{
            $\mathcal{M}_{m\times n}(\F)$, with pointwise addition and scaling is a vector space over $\F$;
        }
    }

    \newco[Vector Space]{co2}{
        Let $\F$ be a field, and $V$ a vector space over $\F$. THen for any $v,w,u\in V$ and $a\in\F$ we have:
        \begin{enumerate}
            \item If $v+w=v+u$, then $w=u$.
            \item If $a\neq 0$ and $av=aw$, then $v=w$.
            \item The element $0\in V$ is unique.
            \item Additive inverses in $V$ are unique. (This means that for each $v\in V$ there is only one element $w\in V$ which satisfies the condition of Axiom 8.)
            \item $(-a)v=-(av)$. In particupar $(-1)v=-v$.
            \item $0v=0$.
            \item $a0=0$.
        \end{enumerate}
    }

    \newd[Subspace]{def23}{
        Let $\F$ be a field and $V$ a vector space over $\F$. We say that a subset $W\subseteq V$ is \tbf{a subspace of} $V$ if $W$ is also a vector space over $|F$ using the same operations defined in $V$.

        \newn{
            $P_n(\F)$ is a subspace of $P(\F)$.
            
            $P_n(\F)$ is a subspace of $P_m(\F)$ if $n<m\in\N$. 
        }
    }

    \tcbcnt{theorem}{18}
    \newt[Subspace]{thm19}{
        Let $V$ be a vector space over a field $\F$. A \tbf{non-empty} subset $W\subseteq V$ is a subspace of $V$ if and only if 

        \begin{enumerate}
            \item For all $v,w\in W$ we have $v+w\in W$.
            \item For all $v\in W$ and $c\in \F$ we have $cv\in W.$
        \end{enumerate}
    }

    \newd[Trivial / Non-Trivial Subspace]{def24}{
        Let $V$ be a vector space over a field $\F$. The subspaces $\{0\}$ and $V$ are called the \tbf{trivial subspaces} of $V$. Any other subspace $W$ of $V$ is called a \tbf{non-trivial subspace of} $V$.

        In particular, we say that a subspace $W$ is a non-trivial subspace of $V$ if $W\neq\{0\}$ and $W\neq V$.
    }
}

\np
\newch[Bases and Dimension]{ch4}{
    \newd[Linear Combinatoin of Vectors]{def25}{
        Let $V$ be a vector space over $\F$, and $v_1,v_2,\ldots,v_k\in V$. A vector of the form $a_1 v_1 + a_2 v_2 +\cdots + a_k v_k \in V$ is called a linear combination of the vectors $v_1,v_2,\ldots, v_k$.
    }

    \newd[Span]{def26}{
        Let $V$ be a vector space over $\F$ and $S\subseteq V$. We define the \tbf{span of} $S$, denoted $\vspan S$, as follows:
        \begin{enumerate}
            \item If $S=\varnothing$ is empty, then $\vspan S=\{0\}$.
            \item Otherwise, $\vspan S=\{a_1 v_1 + a_2 v_2 +\cdots + a_k v_k | a_i\in\F v_i \in S\}$ is the set of all possible linear combinations of vectors from $S$.
        \end{enumerate}
    }

    \newt[Span as Subspace]{thm20}{
        Let $V$ be a vector space over $\F$ and $S\subseteq V$ be \tbf{any} subset of vectors. Then the subset $\vspan S\subseteq V$ is a subspace of $V$.
    }

    \newd[$S$ spans $V$]{def27}{
        Let $V$ be a vector space over $\F$. We say that a subset $S\subseteq V$ is \tbf{a spanning set for} $V$ (or "$S$ \tbf{spans} $V$") if $V=\vspan S$.
    }

    \envbreak
    \newd[Linearly Independnet]{def28}{
        Let $V$ be a vector space over $\F$. We say that a set $S$ is \tbf{linearly independent} if for any vectors $v_1,v_2,\ldots,v_k\in S$: \[c_1v_1+c_2v_2+\cdots+c_kv_k=0\implies c_1=0,c_2=0,\ldots,c_k=0.\] Otherwise, we say that $S$ is \tbf{linearly dependent}.
    }
    \newd[Basis]{def29}{
        Let $V$ be a vector space over $\F$. A subset $\beta\subseteq V$ is called a \tbf{basis} if:
        \begin{enumerate}
            \item $\beta$ spans $V$
            \item $\beta$ is linearly independent.
        \end{enumerate}
        \newn{
            Vector space over $\F$ also has basis.

            Finite spanning set for $V$ also contains basis for $V$.
        }
        \newm{
            Let $\F$ be a field.
            \begin{enumerate}
                \item The set $\{e_1,e_2,\ldots,e_n\}$ is a basis for $\F^n$.
                \item The set $\{E_{ij}|1\leq i\leq m, 1\leq n\}$ is a basis for $\mathcal{M}_{m\times n}(\F)$.
                \item The set $\{1,x,x^2,x^3,\ldots\}$ is a basis for $P(\F)$.
                \item The set $\{1,x,x^2,x^3,\ldots,x^n\}$ is a basis for $P_n(\F)$.
            \end{enumerate}
        }
    }
    \tcbcnt{theorem}{22}
    \newt[Unique Expression from Basis]{thm23}{
        Let $V$ be a vector space over $\mathbb{F}$ and $\beta$ a basis of $V$. Then any $\mathbf{v} \in V$ has a unique expression
        $$
        \mathbf{v}=\sum_{i=1}^n a_i \mathbf{v}_i
        $$
        where $\mathbf{v}_i \in \beta$ and $a_i \in \mathbb{F}$.
    }
    \newt[The Replacement Theorem]{thm24}{
        Suppose that $\beta=\{\mathbf{v}_1,\mathbf{v}_2,...,\mathbf{v}_n\}$ is a basis for $V$ and $I=\left\{\mathbf{w}_1,\mathbf{w}_2,\ldots,\mathbf{w}_k\right\}$ an independent subset of $V.$ Then for any $i\in \{ 1, ..., k\} $, we can obtain a new basis by replacing $i$ elements of $\beta$ with
        $\{\mathbf{w}_1,\ldots,\mathbf{w}_i\}.$
        So after relabelling the elements* $\mathbf{v}_j\in\beta$ we have that the set $\beta_i=\{\mathbf{w}_1,\ldots,\mathbf{w}_i,\mathbf{v}_{i+1},\ldots\mathbf{v}_n\}$ is a basis
        for $V.$

        \newco{co3}{
            Suppose that $V$ is a vector space over $\F$ with a finite basis. Suppose that $\beta$ is any basis of $V$ and $I$ is any independent set. Then $|I|\leq |B|$.
        }

    }
    \newt[Size of Bases]{thm25}{
        Let $V$ be a vector space over a field $\F$. If $V$ has a finite basis, then all bases of $V$ have the same size.
    }
    \newd[Dimension]{def30}{
        Let $V$ be a vector space over $\F$ with a finite basis. We define the \tbf{dimension of} $V$ to be the size of a basis for $V$.

        In this case, we say that $V$ \tbf{is finite dimensional}. Otherwise, we say that $V$ is infinite dimenisonal.
        \newn{
            \begin{enumerate}
                \item $\dim \F^n=n$.
                \item $\dim \mathscr{M}_{m\times n}(\F)=mn.$
                \item $\dim P_n(\F)=n+1.$
                \item $P(\F)$ is infinite dimensional.
            \end{enumerate}


        }
    }
    \newt{12312341}{
        Let $V$ be a finite dimensional vector space over a field $\F$. Suppose taht $S$ is a spanning set and $I$ is a linearly independent set. Then:
        \begin{enumerate}
            \item There exists a subset $\be\subseteq S$ which is a basis for $V$; that is every spanning set contains a basis.
            \item There exists a basis $\be$ of $B$ so that $I\subseteq \be$; that is every linearly independent set can be extended to a basis. 
        \end{enumerate}
    }
    \newco{co4}{
        Let $V$ be a finite dimensional vector space over $\F$. $S$ any spanning set for $V$, $I$ any independent set in $V$, and $\beta$ any basis. Then \[|I|\leq |\beta|\leq |S|.\]
    }

}

\np

\newch[Linear Transformations]{ch5}{
    \newd[Linear Transformation]{d31}{
        Let $V$ and $W$ be vector spaces over $\F$. A map $T:V\ra W$ is called a \tbf{linear transformation} if:
        \begin{align*}
            \Aleq{T(v+w)}{T(v)+T(w)}{for all $v,w\in V$}
            \Aleq{T(cv)}{cT(v)}{for all $v\in V$ and $c\in\F$}
        \end{align*}
        \newn{
            $T(x,y,z)=(2x-4y+z,3x-y+2x)$ is a linear transformation from $\R^3$ to $\R^2$.

            $T(p)=\frac{\D}{\D x}p$ is a linear transformation.
        }
    }

    \newt[Properties of Linearity]{thm26}{
        Let $V,W$ be vector spaces over $\F$.
        \begin{enumerate}
            \item If $T:V\ra W$ is linear, then $T(0_v)=0_w$.
            \item The map $O: V\ra W$ given by $O(v)=0_w$ for all $v\in V$ is linear. This map is called the "zero map."
            \item The map $I_V:V\ra V$ given by $I_V(v)=v$ for all $v\in V$ is linear. This map is called the "identity map."
        \end{enumerate}
    }

    \newt{thm27}{
        Let $V$ e a finite dimensional vector space over $\F$ and $\beta=\{v_1,\ldots,v_n\}$ a basis of $V$. A linear map $T: V\ra W$ is uniquely determined by the values $T(v_1),T(v_2),\ldots,T(v_n)\in W$.
    }

    \newco[Extending by Linearity]{co5}{
        Let $V,W$ be vector spaces over $\F$, and $\beta=\{v_1,\ldots,v_n\}$ a basis for $V$. Given a list of (not necessarily distinct) vectors $w_1,\ldots, w_n\in W$ there is exactly one linear map $T: V\ra W$ so that $W(v_i)=w_i$.

        This map is defined for all $v\in V$ as follows. Writing $v=\displaystyle\sum_{i=1}^n a_i v_i$, we then set $T(v)=\displaystyle\sum_{i=1}^n a_i w_i$.

        This process is called "extending by linearity".
    }

    \newt[Composition of Linear Maps]{thm28}{
        Let $V,W,X$ be vector spaces over $\F$. If $T:V\ra W$ and $S:W\ra X$ are linear maps, then the composition $S\circ T:V\ra X$ is linear.
    }

    \envbreak

    \newt[Null Space / Image]{thm29}{
        Let $V,W$ be vector spaces over $\F$ and $T:V\ra W$ a linear transformation. The sets: 
        \[N(T)=\{v\in V\mid T(v)=0\}\subseteq V\]
        \[\operatorname{im}(T)=\{w\in W\mid w=T(v)\text{ for some }v\in V\}\subseteq W\]

        are subspaces of $V,W$ respectively.

        The subspace $N(T)$ is called the \tbf{null space of} $T$.

        The subspace $\operatorname{im}(T)$ is called the \tbf{image of} $T$.
    }

    \newd[rank]{def32}{
        Let $V,W$ be vector spaces over $\F$ and $T:V\ra W$ linear. We define the \tbf{rank} of $T$ by $\operatorname{rank} T=\dim \operatorname{im}(T)$.
    }

    \newt[The Dimension Theorem]{thm30}{
        Let $V,W$ be finite dimenisonal vector spaces over $\F$. If $T:V\ra W$ linear, then $$\dim V=\dim N(T)+\dim \operatorname{im}(T)$$.
    }

    \newd[$\mathcal{L}(V,W)$]{def33}{
        Let $V,W$ be vector spaces over a field $\F$. We denote by $\mathcal{L}(V,W)=\{T:V\to W\mid T \T{ is linear}\}$ the set of all linear transformations from $V$ to $W$.
        
        We define addition and scaling of transformations by:

        \begin{align*}
            \Aleq{(T+S)(v)}{T(v)+S(v)}{(we can add $T(v),S(v)\in W$)}
            \Aleq{(cT)(v)}{cT(v)}{(we can scale the vector $T(v)\in W$ by $c\in\F$)}
        \end{align*}
    }

    \newt{thm31}{
        Let $V,W$ be vector spaces over a field $\F$. If $S,T\in\mathcal{L}(V,W),$ and $c\in\F$ then $T+S, cT\in\mathcal{L}(V,W)$.
    }

    \newt{thm32}{
        Let $V,W$ be vector spaces over a field $\F$. \begin{enumerate}
            \item The set $\mathcal{L}(V,W)$ is a vector space over $\F$.
            \item If $\dim V=n,\dim W=m,$ then $\dim \mathcal{L}(V,W)=nm$. 
        \end{enumerate}
    }

    \newd[Invertibility]{def34}{
        Let $A,B$ be sets and $f: A\to B$ be a function.
        \begin{enumerate}
            \item We say that $f$ is injective if for all $x,y\in A, f(x)=f(y)\implies x=y.$
            \item We say that $f$ is surjective if $\im f = f(A) = B$.
            \item We say that $f$ is bijective if it is both injective and surjective.
            \item We say that $f$ is \tbf{invertible}, if there exists a function $g: W\to V$ so that $g\circ f=I_A:A\to A$ and $f\circ g=I_B:B\to B$.

            In this case we call the map $g$ the \tbf{inverse} of $f$ and denote it by $f^{-1}$.
        \end{enumerate}
    }

    \newt{thm33}{
        Let $V,W$ be vector spaces over $\F$. If $T: V\to W$ is linear and bijective, then the inverse $T^{-1}: W\to V$ is also linear.
    }

    \newd[Isomorphism]{def35}{
        Let $V,W$ be vector spaces over the field $\F$. We say that a linear map $T: V\to W$ is an \tbf{isomorphism} if it is bijective.

        We say that $V$ is \tbf{isomorphic to} $W$, and write $V\simeq W$, if there exists an isomorphism $T:V\to W$.
    }

    \newt{thm34}{
        Let $V,W$ be vector spaces over $\F$ and $T: V\to W$ be linear. Then $T$ is injective if and only if $N(T)=\{0_v\}$.
    }

    \newt{thm35}{
        Let $V,W$ be finite dimensional vector spaces over $\F$. Then $V\simeq W$ if and only if $\dim V=\dim W$.
    }

    \newt{thm36}{
        Let $V,W$ be finite dimensional vector spaces over the field $\F$.

        If $T: V\to W$ is linear and $\dim V=\dim W$, then the following are equivalent:

        \begin{enumerate}
            \item $T$ is injective.
            \item $T$ is surjective.
            \item $T$ is an isomorphism.
        \end{enumerate}
    }

    \newt{thm37}{
        The maps $R_\theta, \T{proj}_L,\R_L:\R^2\ra\R^2$ are linear.
    }

    \newt{thm38}{
        The maps $R_{\theta, L}, \T{proj}_L, R_P:\R^3\to\R^3$ are linear.
    }

}

\np
\newch[Coordinates]{ch6}{
    \newt{thm39}{
        Let $A\in\mathscr{M}_{m\times n}(\F)$. Define $T_A:\F^n\to \F^m$ by $T_A(x)=Ax$.
        \begin{enumerate}
            \item The map $T_A$ is linear.
            \item The map $F:\mathscr{M}_{m\times n}(\F)\to\mathcal{L}(\F^n,\F^m)$ given by $F(A)=T_A$ is an isomorphism of vector spaces. That is, every linear map $T:\F^n\to \F^m$ is given by matrix multiplication for some matrix $A$.
        \end{enumerate}
    }

    \newd{def36}{
        Let $V$ be a finite dimensional vector space over $\F$. An \tbf{ordered basis} for $V$ is a basis $\beta$, together with a \tbf{fixed} order for listing its elements: $\beta=\{v_1,\ldots,v_n\}$.
    }

    \newt{thm40}{
        Let $V$ be a vctor space over $\F$ of dimension $n$, and $\beta=\{v_1,\ldots,v_n\}$ a basis of $V$. The map $\phi_{\beta}:V\to \F^n$ defined above is an isomorphism of vector spaces.

        We will denote by $[v]_\beta$ the element $\phi_\beta(v)$. I.e. $\phi_\beta(v)=[v]_\beta$.

        We will call a choice of basis on $V$, together with the isomorphism $\phi_beta:V\to \F^n$ a ``\tbf{coordinate system}'' on $V$.

    }

    \newd{def37}{
        Let $V,W$ be finite dimensional vector spaces over $\F$. Let $\beta=\{v_1,\ldots,v_n\}$ be a basis of $V$ and $\gamma=\{w_1,\ldots,w_m\}$ a basis of $W$. For $T\in\mathcal{L}(V,W)$ we define the matrix $[T]_\beta^\gamma\in\mathscr{L}(\F^n,\F^m)$ as follows:
        \begin{enumerate}
            \item The columns of $[T]_\beta^\gamma$ are given by: $[T(v_1)]_\gamma,\ldots,[T(v_n)]_\gamma$.Alternatively: we can express $T(v_j)\in W$ using the basis $\gamma$ to obtain an expression $T(v_j)=\ds\sum_{i=1}^m A_{ij}w_i$. We then define $\bra{[T]_\beta^\gamma}_{ij}=A_{ij}$.
        \end{enumerate}

        When $T: V\to V$ we denote by $[T]_\beta=[T]_\beta^\beta$.
    }

    \newt{thm41}{
        Let $V,W$ be finite dimensional vector spaces over $\F$, $\beta=\{v_1,\ldots,v_n\}$ a basis of $V$, and $\gamma=\{w_1,\ldots,w_m\}$ a basis of $W$. 

        The map $\phi_\beta^\gamma: \mathcal{L}(V,W)\to\mathscr{M}_{m\times n}(\F)$ given by $\phi_\beta^\gamma(T)=[T]_\beta^\gamma$ is an isomorphism of vector spaces.
    }

    \newt{thm42}{
        Let $V,W,X$ be finite dimensional vector spaces over $\F$. Let $\beta=\{v_1,\ldots,v_n\}$ be a basis of $V$, $\gamma=\{w_1,\ldots,w_m\}$ a basis of $W$, and $\delta=\{x_1,\ldots,x_p\}$ a basis of $X$. 

        For all $T\in\mathscr{L}(V,W)$ and $S\in\mathscr{L}(W,X)$ we have that \[[S\circ T]_\beta^\delta=[S]_{\gamma}^\delta[T]_\beta^\gamma.\] In other words, when using compatible coordinate systems, the composition of linear transformations corresponds to matrix multiplication.
    }

    \newt{thm43}{
        Let $V,W$ be finite dimensional vector spaces over $\F$, $T: V\to W$ be a linear map, $\beta=\{v_1,\ldots,v_n\}$ a basis of $V$, and $\gamma=\{w_1,\ldots,w_m\}$ a basis of $W$. Then $T$ is invertible if and only if $[T]_\beta^\gamma$ is invertible.

        Moreover, we have \[[T^{-1}]_\gamma^\beta=\bra{[T]_\beta^\gamma}^{-1}\] if $T$ is invertible.
    }

    \newt{thm44}{
        Let $V,W$ be finite dimensional vector spaces over a field $\F, \beta=\{v_1,\ldots,v_n\}$ a basis of $V$, and $\gamma=\{w_1,\ldots,w_m\}$ a basis of $W$. Then we have \[[T]_\beta^\gamma[v]_\beta=[T(v)]_\gamma\] for all $v\in V$.
    }

    \newt{45}{
        Let $V,W$ be finite dimensional vector spaces over $\F$, $\be, \be'$ bases for $V$, and $\ga, \ga'$ bases for $W$. Let $T:V\to W$ be a linear map.
        \begin{enumerate}
            \item $[v]_{\be'}=[I_V]_{\be}^{\be'}[v]_{\be}$.
            \item $[T]_{\be'}^\ga = [T]^\ga_\be[I_V]^\be_{\be'}$.
            \item $[T]_\be^{\ga'}[T]_\be^\ga. $
            \item $[T]_{\be'}^{\ga'}=[I_W]_{\ga}^{\ga'}[T]_{\be}^{\ga}[I_V]_{\be'}^{\be}$.
        \end{enumerate}
    }

    \newco[Change of Bases]{5}{
        Let $V$ be finite dimensional vector spaces over $\F$, and $\be,\be'$ bases for $V$. If $T:V\to V$ is linear then we have that \[[T]_{\be'}=[I_V]^{\be'}_{\be}[T]_{\be}[I_V]^\be_{\be'}.\] 

        Denoting $[I_V]^{\be}_{\be'}=Q, A=[T]_\be$, and $B=[T]_{\be'},$ and recalling that $[I_V]^\be_{\be'}=\bra{[I_V]^{\be'}_\be}^{-1}$ we have that \[B=Q^{-1}AQ.\]
    }

    \newd[Change of Bases Matrix]{38}{
        Let $V$ be a finite dimensional vector spaces over $\F$, and $\be,\be'$ bases for $V$.

        The matrix $Q=[I_V]^{\be'}_{\be}$ is called the \tbf{change of coordinate matrix from} $\be$ to $\be'$ coordinates.
    }

    \newt{46}{
        Let $V$ be finite dimensional vector spaces over $\F$, and $\be,\be'$ bases for $V$.

        If $Q=[I_V]^{\be'}_{\be}$ is the change of coordinate matrix from $\be$ to $\be'$ coordinates, then $Q^{-1}$ is the change of coordinate matrix from $\be'$ to $\be$ coordinates.
    }
}

\np
\newch[Rank, Invertibility and Systems of Equations]{7}{
    \newd[rank]{39}{
        Let $A\in\mathscr{M}_{m\times n}(\F)$ and $T_A\in\cal{L}(\F^n,\F^m)$ the asscotiated transformation. We define the \tbf{rank of} $A$ denoted $\rank A$ by $\rank A = \rank T_A$.
    }

    \newl{1}{
        Let $V,W$ be finite dimensional vector spaces over $\F$, $\be=\{v_1,\ldots,v_n\}$ and $\ga=\{w_1,\ldots,w_n\}$ bases of $V,W$ respectively, and $T:V\to W$ a linear map. We have $\im(T)=\vspan[T(v_1),\ldots,T(v_n)]$.

        As a consequence we have that if $A\in\mathscr{M}_{m\times n}(\F)$, then $\im(T_A)$ is spanned by the columns of $A$.
    }

    \newt{47}{
        Let $V,W,X$ be finite dimensional vector spaces over $\F$ and $T:V\to W$, $S:W\to X$ linear transformations.
        \begin{enumerate}
            \item $\im(S\circ T)\subseteq \im S$.
            \item $\im(S\circ T)\subseteq S(\im T)$.
            \item If $S$ is an isomorphism, then $\im S\circ T\simeq\im T$.
            \item If $T$ is an isomorphism, then $\im S\circ T=\im S$.
        \end{enumerate}
    }

    \newco{6}{
        Let $V,W,X$ be finite dimensional vector spaces over $\F$ and $T:V\to W$, $S: W\to X$ linear transformations.
        \begin{enumerate}
            \item $\rank T \le \min[\dim V, \dim W]$.
            \item \(\rank(S\circ T)\le \rank S\).
            \item \(\rank(S\circ T)\le \rank T\).
            \item If $S$ is an isomorphism, then $\rank S\circ T=\rank T$.
            \item If $T$ is an isomorphism, then $\rank S\circ T=\rank S$.
        \end{enumerate}
    }

    \newt[Rank of change of bases]{48}{
        Let $V,W$ be finite dimensional vector spaces over $\F$, $\be=\{v_1,\ldots,v_n\}$ and $\ga=\{w_1,\ldots,w_m\}$ bases of $V,W$ respectively, and $T: V\to W$ a linear map. Then we have $\rank T=\rank[T]^\ga_\be$.
    }

    \newco{7}{
        Let $A\in\mathscr{M}_{m\times n}(\F), B\in\mathscr{M}_{n\times k}(\F)$.
        \begin{enumerate}
            \item $\rank A\le \min\{m,n\}$.
            \item $\rank AB\le \rank A$.
            \item $\rank AB\le\rank B$.
            \item If $A$ is invertible, then $\rank AB=\rank B$.
            \item If $B$ is invertible, then $\rank AB=\rank A$.
        \end{enumerate}
    }

    \newco[Invertibility and Rank]{8}{
        A square matrix $A\in\mathscr{M}_{n\times n}(\F)$ is invertible if and only if $\rank A= n$.
    }

    \newd[Elementary Matrix]{40}{
        Let $E\in\mathscr{M}_{n\times n}(\F)$ be a matrix obtained from $1_n$ by performing a row/column operation. We call $E$ an \tbf{elementary matrix}. We say that $E$ is of Type I, II, or, III if it is obtianed using a Type I, II, or III operation.
    }

    \newt{49}{
        Let $A\in\mathscr{M}_{m\times n}(\F)$ and $E\in\mathscr{M}_{n\times n}(\F)$, $E'\in\mathscr{M}_{m\times m}(\F)$ be elementary matrices. Set $B=AE$ and $C=E'A$.
        \begin{enumerate}
            \item If $E$ is obtained from $1_n$ by performing a column operation, then $B$ is obtained from $A$ by performing \tbf{the same} column operation.
            \item If $E$ is obtained from $1_n$ by performing a row operation, then $B$ is obtained from $A$ by performing \tbf{the same} row operation.
            \item $E$ is invertible, and its inverse is an elementary matrix of the same type. (The same is true of $E'$).
        \end{enumerate}
    }

    \newt{50}{
        Let $A\in\mathscr{M}_{m\times n}(\F)$ and $E\in\mathscr{M}_{n\times n}(\F)$, $E'\in\mathscr{M}_{m\times m}(\F)$ be elementary matrices. Set $B=AE$ and $C=E'A$.
        \begin{enumerate}
            \item $\rank A=\rank B$.
            \item $\rank A=\rank C$.
            \item $Ax=y$ if and only if $Cx=Cy$. In other words, row operations do not change the solutions in a system.
            \item $\im (T_A)=\im (T_B)$. In other words, column operations do not change the image.
        \end{enumerate}
    }

    \newd[RREF]{41}{
        We say a matrix $A$ is in \tbf{\underline{row-reduced echelon form}} if \tit{all} of the following conditions are met:
        \begin{enumerate}
            \item All zero rows are at the bottom of the matrix $A$.
            \item The first non-zero entry of a non-zero row is 1. (Such entries are called "leding 1's".)
            \item The leading 1's move to the right, as we go down the rows of $A$.
            \item All entries above and below a leading 1 are 0.
        \end{enumerate}

        We use the abbreviation "RREF" for "row-reduced echelon form".
    }

    \newt[Gaussian Elimination]{51}{
        Let $A\in\mathscr{M}_{m\times n}(\F)$.
        \begin{enumerate}
            \item If the matrix consists entirely of 0's, stop. It's already row-reduced.
            \item Find the first column with a non-zero entry and move the corresopnding row to the top. (We will call the first non-zero entry $a$.)
            \item Divide the row by the number $a$ to obtain a leading one.
            \item Subtract multiples of this row from the rows above and below, in order to make each entry above and below the leading 1 equal to 0.
            \item Repeat 1-4 on the matrix consisting of the remaining rows.
        \end{enumerate}

        At the end of the process, we obtain a matrix $R\in\mathscr{M}_{m\times n}(\F)$ which is in RREF.

        As a consequence, every matrix has a RREF.
    }

    \newl{2}{
        Let $A\in\mathscr{M}_{m\times n}(\F)$ and let $R\in\mathscr{M}_{m\times n}(\F)$ be its RREF.
        \begin{enumerate}
            \item If $k$ is the number of zero rows in $R$, then the columns of $R$ consisting the leading ones are the first $m-k$ elements $\{e_1,\ldots, e_{m-k}\}$ of the standard basis $\{e_1,e_2,\ldots,e_m\}$ for $\F^m$.
            \item $\rank A=\rank R=$\# of leading 1's.
            \item $\rank A=\rank R=$\# of non-zero rows in $R$.
        \end{enumerate}
    }

    \newco{9}{
        Let $A\in\mathscr{M}_{m\times n}(\F)$. Then there exists an invertible matrix $b\in\mathscr{M}_{m\times m}(\F$) so that $BA=R$ is in RREF.
    }

    \newco{10}{
        Let $A\in\mathscr{M}_{n\times n}(\F)$. Then $A$ is invertible if and only if the RREF of $A$ is $1_n$.
    }

    \newt{52}{
        Let $A\in\mathscr{M}_{m\times n}(\F)$ be a matrix and $r=\rank A$.

        If $r=0$, then $A=O_{mn}$ is the zero matrix.

        If $r>0$, then there exists a sequence of row operations an column operations, which transform $A$ into a matrix of the form $\left.\left(\begin{array}{c|c}{{\mathrm{I}_{r}}}&{0}\\\hline{0}&{0}\end{array}\right.\right)$.
    }

    \newt{53}{
        Let $A\in\mathscr{M}_{m\times n}(\F)$, $R$ denote its $RREF$, and $T_A:\F^n\to\F^m$ te associated linear transformation. Let $a_1,\ldots, a_n$ denote the columns of $A$.

        \begin{enumerate}
            \item The set of $n-r$ ``basic solutions'' for to $Ax=0$ is a basis for $N(T_A)$.
            \item If the leading 1's in $R$ appear in columns $k_1,\ldots, k_r$, then the columns $a_{k_1},a_{k_2},\ldots,a_{k_r}$ of $A$ provide a basis for $\im(T_A)$.
            \item If we apply the Gaussian Algorithm replacing ``row operations'' with ``column operations'' at every step, then the non-zero columns of the resulting matrix provide a basis for $\im(T_A)$.
        \end{enumerate}
    }

    \newt[Invertibility]{54}{
        Let $A\in\mathscr{M}_{n\times n}(\F)$. Then $A$ is invertible if and only if $A$ is a product of elementary matrices.
    }

    \newt[Computing the Inverse]{55}{
        Let $A\in\mathscr{M}_{n\times n}(\F)$. Consider the augmented matrix $\left(A|I_n\right)$. Row reduce the augmented matrix to obtain $\bra{R\mid B}$, where $R$ is the RREF of $A$.

        Then $A$ is invertible if and only if this procedure results in the augmented matrix $\bra{I_n\mid B}$. In this case, $B=A^{-1}$.
    }
}


\np
\newch[Determinants]{8}{
    \newd{42}{
        Let $A=\bra{\begin{array}{cc}a & b \\ c & d\end{array}}\in\mathscr{M}_{2\times2}(\F)$. We define the \tbf{determinant} of $A$ by $\det \bra{\begin{array}{cc}a & b \\ c & d\end{array}}=ad-bc\in\F$.
    }

    \newt{56}{
        The matrix $A=\bra{\begin{array}{cc}a & b \\ c & d\end{array}}\in\mathscr{M}_{2\times2}(\F)$ is invertible if and only if $\det A=ad-bc\neq0$.

        In this case $A$ is invertible we have $A^{-1}=\frac{1}{ad-bc}\bra{\begin{array}{cc}d & -b \\ -c & a\end{array}}.$
    }

    \newd{43}{
        Let $A\in\mathscr{M}_{n\times n}(\F)$. For each $i,j\in\{1,\dots,n\}$ we define $\tilde{A}_{ij}$ to be the $(n-1)\times (n-1)$ matrix obtained by deleting row $i$ and column $j$ of $A$.

        We define $\det A\in\F$ as follows:
        \begin{enumerate}
            \item For $n=1$, $A=(a)\in\F,$ and we set $\det(a)=a$.
            \item For $n=2$, $\det\bra{\begin{array}{cc} a & b \\ c & d\end{array}}=ad-bc$
            \item For $n\ge2$, we set $\det A=\ds\sum_{j=1}^n (-1)^{1+j} A_{1j}\det\bra{\tilde{A}_{1j}}$
        \end{enumerate}
    }

    \newt{57}{
        Consider the determinant as a function on the rows of a matrix.

        If $\tbf{r}_1,\dots,\tbf{r}_n,\tbf{a},\tbf{b}\in\F^n$ and $c\in\F$, then we have: \[\det\bra{\begin{array}{c}\tbf{r}_1\\\vdots\\ c\tbf{a+b}\\\vdots\\ \tbf{r}_n\end{array}}=c\det\bra{
            \begin{array}{c}
                \tbf{r}_1\\\vdots\\ \tbf{a}\\\vdots\\ \tbf{r}_n
            \end{array}} + \det\bra{
                \begin{array}{c}
                    \tbf{r}_1\\\vdots\\ \tbf{b}\\\vdots\\ \tbf{r}_n
                \end{array}
            }.
        \]

        In other words, for each $i\in\{1,\dots,n\}$, the determinant is a linear function of row $i$, \tbf{with all the other rows fixed}.
    }

    \newt{58}{
        Let $A\in\mathscr{M}_{n\times n}(\F)$ with $n\ge2$.
        \begin{enumerate}
            \item If $A$ has a row of $0$'s, then $\det A=0$.
            \item For any $i\in\{1,\dots,n\}$, $\det A=\ds\sum_{j=1}^n (-1)^{i+j}A_{ij}\det\bra{\tilde{A}_{ij}}$. (This is called the ``row $i$ expansion of $\det A$''.)
            \item If $A$ has two identical rows, then $\det A=0$.
            \item If $B$ is obtained from $A$ by swapping two rows, then $\det B=-\det A$.
            \item If $B$ is obtained from $A$ by scaling row $i$ by $a\in\F$, then $\det B=a\det A$.
            \item If $B$ is obtained from $A$ by adding a multiple of one row to another, then $\det B=\det A$. (Here $n\ge2$ so that the statement makes sense.)
            \item If $A$ is upper (or lower) triangular, then $\det A$ is the product of the diagonal entries of $A$.
        \end{enumerate}
    }

    \newt{59}{
        Let $A\in\mathscr{M}_{n\times n}(\F)$. If $\rank A<n,$ then $\det A=0$.
    }

    \newt{60}{
        Let $E\in\mathscr{n\times n}(\F)$ be an elementary matrix.
        \begin{enumerate}
            \item If $E$ is of Type I, then $\det E=-1$.
            \item If $E$ is of Type II, and scales by $a\in F$, then $\det E=a$.
            \item If $E$ is of Type III, then $\det E=1$.
        \end{enumerate}
    }

    \newt{61}{
        Let $A,B\in\mathscr{M}_{n\times n}(\F)$. Then $\det(AB)=\det(A)\det(B)$.
    }

    \newt{62}{
        Let $A\in\mat[n]{n}$. Then $A$ is invertible if and only if $\det A\neq0$.
    }

    \newt{63}{
        Let $A\in\mat[n]{n}$. Then $\det(A^t)=\det A$.
    }

    \newco{11}{
        Let $A\in\mat[n]{n}$, then $\det(A)=\ds\sum_{i=1}^n(-1)^{i+1}A_{i1}\det\bra{\tilde{A}_{i1}}$. (This is called the ``column $j$ expansion of $\det A$'').
    }

    \newt{64}{
        Let $A,B\in\mat[n]{n}$. If $A\sim B$, then $\det A=\det B$.
    }

    \newd{44}{
        Let $V$ be finite dimensional vector space over $\F$, and $T:V\to V$ a linear map. We define \[\det T=\det[T]_{\be},\] where $\be$ is any basis of $V$.
    }

    \newt[Geometric meaning of Determinants]{65}{
        Let $:\R^2\to\R^2$ be linear. Then $|\det T|=$Area($P$), where $P$ is the parallelogram spanned by $T(\tbf{$e_1$}),T(\tbf{$e_2$})$.
    }

    \newt{66}{
        Let $T:\R^3\to\R^3$ be linear. Then $|\det T|=$Vol($P$), were $P$ is the parallelepiped spanned by $T(\tbf{$e_1$}),T(\tbf{$e_2$}),T(\tbf{$e_3$})$.
    }

    \newt{67}{
        Let $T:\R^n\to\R^n$ be linear. Then $|\det T|$=Vol($P$), where $P$ is the $n$-parallelepiped spanned by $T(\tbf{$e_1$}),\dots,T(\tbf{$e_n$})$.
    }
}


\np
\newch[Diagonalization]{9}{
    \newd{45}{
        Recall, we say that a square matrix $D\in\mat[n]{n}$ is diagonal if $D_{ij}=0$ for $i\ne j$.

        We will denote the diagonal matrix $\left.D=\left(\begin{array}{cccc}d_{11}&0&\cdots&0\\0&d_{22}&\cdots&0\\\vdots&\vdots&\ddots&\vdots\\0&0&\cdots&d_{nn}\end{array}\right.\right)$ by $D=\diag (d_{11},\ldots, d_{nn})$.

        For example $\diag(1,2,3)$ denotes the diagonal matrix $\left.\left(\begin{array}{ccc}1&0&0\\0&2&0\\0&0&3\end{array}\right.\right)$.
    }

    \newd{46}{
        Let $V$ be a vector space over $\F$ and $T:V\to V$ a linear map.

        We say that $\tbf{v}\in V$ is \tbf{an eigenvector for} $T$ if $\tbf{v}\neq \tbf{0}$ and $T(\tbf{v})=\la \tbf{v}$ for some $\la\in\F$.

        We call the scalar $\la\in\F$ an \tbf{eigenvalue} of $T$.

        Let $E_\la=\{\tbf{v}\in V\mid T(\tbf{v}=\la \tbf{v})\}$. We call $E_\la$ the \tbf{eigenspace for eigenvalue $\la$}.
    }

    \newt{68}{
        Let $V$ be a vector space over $\F$ and $T:V\to V$ a linear map. For any $\la\in\F$, the set $E_\la$ is a subspace.
    }

    \newco{12}{
        Let $V$ be a vector space over $\F$ and $T:V\to V$ a linear map. For any $\la\in\F$ we have $E_\la=N(\la \tbf{I}_V-T)$.
    }

    \newd{47}{
        Let $V$ be a finite dimensional vector space over $\F$ and $T:V\to V$ a linear map.

        Set $C_T(x)=\det(x\tbf{I}_V-T)$. We call $C_T$ the \tbf{characteristic polynomial} of $T$.

        Similarly, for $A\in\mat[n]{n}$ we define $C_A(x)=\det(x\tbf{I}_V-A)$, and call $C_A$ the characteristic polynomial of $A$.
    }

    \newt{69}{
        Let $V$ be a finite dimensional vector space over $\F$ and $T:V\to V$ a linear map. Let $n=\dim V$. 
        \begin{enumerate}
            \item $C_T$ is a polynomial of degree $n$ and the coefficient of $x^n$ is 1.
            \item $\la\in\F$ is an eigenvalue of $T$ if and only if $C_T(\la)=0$. That is, the roots of $C_T$ are precisely the eigenvalues of $T$.
            \item For all eigenvalues $\la\in\F$, the corresponding eigenvectors can be found by solving the homogeneous system $(\la \tbf{I}-T)(\tbf{x})=\tbf{0}_V$.
        \end{enumerate}
    }

    \newd{48}{
        Let $V$ be a finite dimensional vector space over $\F$ and $T:V\to V$ a linear map. We say that $T$ is \tbf{diagonalizable} if there exists a basis $\be$ of $B$ so that $[T]_\be$ is a diagonal matrix.

        We say that $A\in\mat{n}$ is \tbf{diagonalizable} if there eixsts an invertible matrix $P$ and diagonal matrix $D$ so that $A=PDP^{-1}$.
    }

    \newt[Alternate Characterizations of Diagonalizability]{70}{
        1. Let $V$ be a finite dimensional vector space over F and $T{: }V\to V$ a linear map. $T$ is diagonalizable if and only if there exists a basis of $V$ consisting of eigenvectors for $T.$ 

        $2. A\in\mathscr{M}_{\mathbf{n}\times\mathbf{n}}$(F) is diagonalizable if and only if $A$ is similar to a diagonal matrix.

        $3. A\in\mathscr{M}_{\mathbf{n}\times\mathbf{n}}(\mathcal{F})$ is diagonalizable if and only if $T_A{:}\mathbb{F}^n\to\mathbb{F}^n$ is a diagonalizable linear map.
    }

    \newt[Test for Diagonalization]{71}{
        Let $V$ be a finite dimensional vector space over $\F$ and $T: V\to V$ a linear map.

        $T$ is diagonalizable if and only if all the conditions here are met:

        1. The characteristic polynomial $C_T(x)=\prod_{i=1}^k\left(x-\lambda_i\right)^{m_i}$ splits over $\mathbb{F}$.*

        2. For each eigenvalue $\lambda_i$ we have $m_i=\operatorname{dim} E_i$.

        If these conditions are met, then we obtain a diagonalizing basis $\beta$ as follows:

        1. For each eigenvalue $\lambda_i$ find a basis $\beta_i$ of $E_i=N\left(\lambda_i \mathrm{I}_V-T\right)$ by solving the corresponding system.

        2. Set $\beta=\beta_1 \cup \cdots \cup \beta_k$.

        3. Then $[T]_\beta=\operatorname{diag}\left(\lambda_1, \ldots, \lambda_n\right)$

    }
    \newco[The "Matrix" Test for Diagonalization]{13}{
        Let $A \in \mathscr{M}_{\mathbf{n} \times \mathbf{n}}(\mathbb{F})$. Then $A$ is diagonalizable if and only if all the following conditions are met:
    
        1. The characteristic polynomial $C_A(x)=\prod_{i=1}^k\left(x-\lambda_i\right)^{m_i}$ splits over $\mathbb{F}$.
    
        2. For each eigenvalue $\lambda_i$ we have $m_i=\operatorname{dim} E_i$.
        
        If these conditions are met, then we obtain $P, D$ as follows:
    
        1. For each eigenvalue $\lambda_i$ find a basis $\beta_i$ of $E_i=N\left(\lambda_i \mathrm{I}-A\right)$ by solving the corresponding system.
        
        2. Moreover, the matrix $P$ has columns $\mathbf{v}_1, \mathbf{v}_2, \ldots \mathbf{v}_n$, where $\mathbf{v}_i \in \beta$ are eigenvectors (expressed as columns in "standard coordinates"). The matrix $D=\operatorname{diag}\left(\lambda_1, \ldots, \lambda_n\right) .^{. *}$
    
    }

    \newd{49}{
        Suppose that $T:V\to V$ is linear and $\la\in\F$ is an eigenvalue. Then we know that $\la$ is a root of $C_T$, so we have $C_T(x)=(x-\la)^m p(x)$ for some $m\in\N$ and $p\in \T{P}(\F)$ so that $p(\la)\ne 0$.

        We call $m$ the multiplicity of $\la$.
    }

    \newl{3}{
        Let $V$ be a finite dimensional vector space over $\F$ and $T: V\to V$ a linear map.

        If $\la$ is an eigenvalue of $T$, then we have $1\le \dim E_\la \le m_\la$. 
    }

    \newl{4}{
        Let $V$ be a finite dimensional vector space over $\F$ and $T: V\to V$ a linear map.

        If $\la_1, \la_2$ are distinct eigenvalues for $T$, and $S_1\subseteq E_1$, $S_2\subseteq E_2$ linearly independent subsets of eigenvectors, then $S_1\cup S_2$ is linearly independent.
    }

    \newt[Invariant Subspaces and Diagonalization]{72}{
        Let $V$ be a finite dimensional vector space over $\F$ and $T: V\to V$ be a linear map. The following two conditions are equivalent: 
        \begin{enumerate}
            \item $T$ is diagonalizable.
            \item There exists one-dimensional invariant suspaces $W_1,\ldots,W_n$ so that $V=W_1\oplus\cdots\oplus W_n$.
        \end{enumerate}
    }
    
}











\np

\newch[Matrix Algebra (Appendix A)]{ch999}{
    \newd[Matrix Multiplication]{def86}{
        Let $A\in\mathscr{M}_{m\times n}(\F)$ and $B\in\mathscr{M}_{n\times k}(\F)$. We define their product $AB\in\mathscr{M}_{m\times k}(\F)$ as follows: for $i\in\{1,\ldots,m\}$ and $j\in\{1,\ldots,k\}$ the $ij$-entry of the product $AB$ is given by 
        
        \[(AB)_{ij}=\sum_{l=1}^n A_{il}B_{lj}.\]

        \newn{
            $AB\neq BA$.
        }
    }

    \newd[Special Matrices]{def87}{
        For each $n,m\in\N$ we define the following matrices:
        \begin{enumerate}
            \item $O_{m,n}\in\mathscr{M}_{m\times n}(\F)$ - the matrix consisting of all 0's. In other words $(O_{m,n})_{i,j}=0$ for all $i\in\{1,\ldots,m\}$ and $j\in\{1,\ldots,n\}$. 
            \item $I_n\in\mathscr{M}_{n\times n}(\F)$ - the matrix with 1's on the diagonals, and 0 in all other entries. In other words 
            
            $$(I_n)_{ij}=\begin{cases}1&\text{if }i=j\\0&\text{if }i\neq j\end{cases}$$\\
        \end{enumerate}

        \newm{
            $$O_{2,3}= \begin{pmatrix}0&0&0\\0&0&0\\\end{pmatrix}$$
        
            $$I_3=\begin{pmatrix}1&0&0\\0&1&0\\0&0&1\\\end{pmatrix}$$
        }
    }

    

    \newt{thm124}{
        Let $\mathbb{F}$ be a field, $A, A_1, A_2 \in \mathscr{M}_{\mathbf{m} \times \mathbf{n}}(\mathbb{F}), B, B_1, B_2 \in \mathcal{M}_{n \times k}(\mathbb{F}), C \in \mathcal{M}_{k \times p}(\mathbb{F})$ and $c \in \mathbb{F}$.
        
        1. $A(B C)=(A B) C$
        
        2. $\left(A_1+A_2\right) B=A_1 B+A_2 B$
        
        3. $A\left(B_1+B_2\right)=A B_1+A B_2$
        
        4. $I_m A=A=A I_n$
        
        5. $\mathrm{O}_{r m} A=\mathrm{O}_{r n}$ for any 
        $r \in \mathbb{N}$.
        
        6. $A(c B)=c(A B)=\left(c \mathrm{I}_m\right) A B=A B\left(c \mathrm{I}_k\right)=A\left(c \mathrm{I}_n\right) B$.
    }

    \newd[Invertibility]{def88}{
        Let $A\in\mathscr{M}_{n\times n}(\F)$. We say that $A$ is \tbf{invertible} if there exists a matrix $B\in\mathscr{M}_{n\times n}(\F)$ so that $AB=I_n=BA$. 
    }

    \newt{thm125}{
        Let $A,B\in\mathscr{M}_{n\times n}(\F)$. 
        \begin{enumerate}
            \item If $A$ is invertible, then the inverse of $A$ is unique.
            \item If $A$ is invertible, then $A^{-1}$ is also invertible.
            \item If $A$ and $B$ are invertible, then $AB$ is invertible.
            \item $I_n$ is invertible.
            \item If $AB=I_n$, then $A$ is invertible and $B=A^{-1}$.
        \end{enumerate}
    }

    \newd[$A^t$]{def89}{
        Let $A\in\mathscr{M}_{m\times n}(\F)$. We define the matrix $A^t\in\mathscr{M}_{n\times m}(\F)$  by:

        \[(A^t)_{ij}=A_{ji}.\]

        In other words, to obtain $A^t$ we ``swap the rows and columns of A."
    }

    \newd[Symmetric]{def90}{
        We say that $A\in\mathscr{M}_{n\times n}(\F)$ is \tbf{symmetric} if $A^t=A$. We denot ethe set of all symmetric matrices by \tbf{Sym}$_n(\F)$.

        We say that $A\in\mathscr{M}_{n\times n}(\F)$ is \tbf{skew-symmetric} if $A^t=-A$. We denote the set of all skew-symmetric matrices by \tbf{Sk}$_n(\F)$.
    }

    \newt{thm126}{
        Let $A, B \in \mathscr{M}_{\mathbf{m} \times \mathbf{n}}(\mathbb{F}), C \in \mathscr{M}_{n \times k}(\mathbb{F})$ and $c \in \mathbb{F}$.
        
        1. $(A+B)^t=A^t+B^t$
        
        2. $(c A)^t=c A^t$
        
        3. $\left(A^t\right)^t=A$
        
        4. $(A C)^t=C^t A^t$.
        
        5. In the case that $m=n$, we also have that if $A \in \mathscr{M}_{\mathbf{n} \times \mathbf{n}}(\mathbb{F})$ is invertible, then $A^t \in \mathscr{M}_{\mathbf{n} \times \mathbf{n}}(\mathbb{F})$ is invertible and $\left(A^t\right)^{-1}=\left(A^{-1}\right)^t$.
    }

    \newd[Diagonal and Triangular]{def91}{
        We say that $A$ is \tbf{diagonal} if $A_{ij}=0$ for all $i\neq j$. 

        Let $A\in\mathscr{M}_{n\times n}(\F)$. We say that $A$ is \tbf{upper triangular} if $A_{ij}=0$ for all $i>j$. This means that all entries below the diagonal of $A$ must be 0.

        Similarily, we say that $A$ is \tbf{lower triangular} if $A_{ij}=0$ for all $i<j$. This means that all entries above the diagonal of $A$ must be 0.

        We say that $A$ is \tbf{strictly upper-triangular} if $A_{ij}=0$ for all $i\geq j$. 
    }
    
}

\end{document}