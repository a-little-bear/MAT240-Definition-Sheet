\documentclass[12pt]{alittlebear}

\def\name{Joseph Siu}
\def\course{MAT240: Algebra I}
\def\headername{Definition Sheet}
\def\headernum{}

\begin{document} 

\newch[Fields and Polynomials]{ch1}{
    
    \newd[Field]{def1}{
        A field $\F$ is a set with two special elements "$0\in \F$" and "$1\in \F$" and two operations "$+$" and "$\cdot$" which satisfy the following axioms.
        \begin{enumerate}
            \item (Commutativity) For all $x,y\in\F$ we have: $x+y=y+x$ and $x\cdot y=y\cdot x$.
            \item (Associativity) For all $x,y,z\in\F$ we have: $(x+y)+z=x+(y+z)$ and $(x\cdot y)\cdot z=x\cdot(y\cdot z)$.
            \item (Distributivity) For all $x,y,z\in\F$ we have: $x\cdot(y+z)=x\cdot y+x\cdot z$.
            \item (Identities) For all $x\in\F$ we have: $x+0=x$ and $x\cdot 1=x$.
            \item (Inverses) For all $x\in\F$ there exists $y\in\F$ such that $x+y=0$. For all $x\in\F\setminus\{0\}$ there exists $z\in\F$ such that $x\cdot z=1$.
        \end{enumerate}
        \newn{
            $\Q$ is a field where $\Z$ is not a field becasue of the absence of some multiplicative inverse.
        }
    }
    \newco[Field]{coro1}{
        Let $\F$ be a field and $a,b,c\in\F$.

        \begin{enumerate}
            \item If $a+c=b+c$, then $a=b$.
            \item If $c\neq 0$ and $c\cd a = c\cd b$, then $a=b$.
            \item The field elements $0,1$ are unique.
            \item The elements $y$ and $z$ from Axoim $5$ are unique. (From now on, we will denote the additive inverse of $x$ by $-x$, and the multiplicative inverse of $x$ by $x^{-1}.$)
            \item $a\cd 0=0$.
            \item $(-a)\cd(b)=-(a\cd b)=(a)\cd(-b).$
            \item $-(-a)=a$. If $a\neq0$, then $\bra{a^{-1}}^{-1}=a$.
            \item If $a\cd b=0$, then $a=0$ or $b=0$.
        \end{enumerate}
    }
        
    \envbreak
    \newt[Equivalence]{thm1}{
        \begin{enumerate}
            \item $\sim$ is an equivalence relation.
            \item $a\sim b$ if and only if $a$ and $b$ have the same remainder when divided by $n$.
            \item There are exactly $n$ equivalence classes for this relation: $[0],[1],\ldots,[n-1]$ - one for each possible remainder for division by $n$.
        \end{enumerate}
    }

    \newd[$\Z_n$]{def2}{
        Let $\Z_n=\{[0],[1],\ldots,[n-1]\}$ be the est of equivalence classes for this equivalence relation. We define $+,\cd$ on $\Z_n$ as follows:
        \[[a]+[b]=[a+b],\]
        \[[a]\cd[b]=[a\cd b].\]
    }

    \envbreak
    \newt[Quadratic Formula]{thm2}{
        Let $a,b,c\in\R$. The quadratic equation $ax^2+bx+c=0$ (where $a\neq0$) has:
        \begin{enumerate}
            \item Solutions $x=\frac{-b\pm\sqrt{b^2-4ac}}{2a}$ if $b^2-4ac\geq0.$
            \item No solutions if $b^2-4ac<0.$
        \end{enumerate}
    }

    \newd[Complex]{def3}{
        Let $i=\sqrt{-1}$. I.e. $i$ is a number with the property that $i^2=-1$.

        Let $\C=\{a+bi \mid a,b\in\R\}.$ We call $\C$ the set of complex numbers and we define addition and multiplication $+,\cd$ in the obvious ways:
        \[(x+yi)+(a+bi)=(x+a)+(y+b)i\]
        \[(x+yi)\cd(a+bi)=ax+bi+ayi+byi^2=(ax-by)+(ay+bx)i\]
        \newn{
            $\C$ is a field.
        }

        Given a complex number $z=x+yi$, we define its \underline{\textbf{conjugate}} by: \[\overline{z}=x-yi.\]

        We define the \underline{\textbf{length (or modulus)}} of a complex number by: \[|z|=\sqrt{x^2+y^2}.\] 

        Note that we in the $xy$-plane, we obtain $\overline{z}$, the conjugate of $z$, by reflecting $z$ in the $x$-axis, and the length of a complex number is just the usual distance from $z$ to the origin in the $xy$-plane.
    }

    \newt[Complex]{thm3}{
        For any $z,w\in\C$ we have:
        \begin{enumerate}
            \item $\overline{z+w}=\overline{z}+\overline{w}$.
            \item $\overline{z\cd w}=\overline{z}\cd\overline{w}$.
            \item $\overline{\frac{z}{w}}=\frac{\overline{z}}{\overline{w}}$ (provided $w\neq0$).
            \item $\overline{\overline{z}}=z$.
            \item $z\overline{z}=|z|^2$.
            \item $z^{-1}=\frac{\overline{z}}{|z|^2}$ (provided $z\neq0$).
            \item $|zw|=|z||w|$.
            \item $\abs{\frac{z}{w}}=\frac{|z|}{|w|}$ (provided $w\neq0$).
            \item $|z+w|\leq |z| + |w|$ ("Triangle nequality for Complex Numbers").
        \end{enumerate}
    }

    \envbreak
    \newd[Polar Form]{def4}{
        For $z=x+yi$, we define its \underline{\textbf{polar form}} as $z=re^{i\theta}$, where $r=|z|=\sqrt{x^2+y^2}$ and $\theta$ is the angle between $z$ and the positive $x$ axis (measured counterclockwise, in radians). The angle $\theta$ is called the \underline{\textbf{argument}} of $z$, and $r$ is called the \underline{\textbf{length (or modulus)}} of $z$.
    }
    \newt[Polar Form]{thm4}{
        Let $z=re^{i\theta}$, $w=Re^{i\phi}$.
        \begin{align*}
            zw=rRe^{i(\theta+\phi)}\\
            z^n=r^ne^{in\theta}\\
        \end{align*}
    }

    \envbreak
    \newd[Polynomial]{def5}{
        A polynomial $p$ with coefficient from $\F$ is an expression \[p(x)=c_0+c_1 x + c_2 x^2 +\cdots + c_n x^n\] where $c_i\in\F$. We call the field elements $c_0,\ldots,c_n$ the "coefficients" of $p$.

        The largest exponent $n$ so that $c_n\neq0$ is called the \tbf{degree} of $p$, and we typically write $\deg p=n$. Constant polynomials are degree 0.

        The set of all polynomials over $\F$ is denoted by $P(\F)$.

        The set of all polynomials of degree \tbf{less than or equal} to $n$ is denoted by $P_n(\F)$.
    }
    \newt[Polynomial]{thm5}{
        Let $\F$ be a field, and $f,g\in P(\F)$ be non-zero polynomials. Then there exist unique polynomials $q,r\in P(\F)$ so that:
        \begin{enumerate}
            \item $f(x)=q(x)g(x)+r(x)$.
            \item $\deg r<\deg g$ if $\deg g\neq0$.
            \item $r=0$ if $\deg g=0$.
        \end{enumerate}
    }
    \newd[Polynomial Cont.]{def6}{
        Let $\F$ be a field and $f,g\in P(\F)$. We say that $g$ divides $f$ if $f(x)=q(x)g(x)$ for some polynomial $q\in P(\F)$.

        We say that a non-constant polynomial $p\in P(\F)$ is "irreducible" if we \tbf{cannot} express $p$ as a product of polynomials of smaller degree.

        I.e. $p$ is irreducible if we \tbf{cannot} write $p(x)=g(x)q(x)$ for any polynomials $g,q\in P(\F)$ with the property that both $\deg g, \deg q < \deg p$. 

        \newn{
            $f(x)=x^2-2$ is irreducible over $\Q$ but not over $\R$.
        }
    }

    \newt[Polynomial Cont.]{thm6}{
        Let $\F$ be a field, $p\in P(\F)$ and $\deg p\geq 1$. Then $a\in \F$ is a root of $p$ if and only if $x-a $ divides $p$. 
    }
    \envbreak

    \newt[Fundamental Theorem of Algebra]{thm7}{
        Every non-constant polynomial has a root over $\C$.

        In fact, every non-constant polynomial factors completely into a product of linear terms over $\C$.
    }
}

\newpage
\tcbcnt{definition}{8}
\tcbcnt{theorem}{10}
\newch[Linear Systems]{ch2}{
    \newd[Linear]{d9}{
        Let $\F$ be a field and $b,c_1,\ldots,c_n\in\F$. An equation in the variables $x_1,\ldots,x_n$ is called \tbf{linear} if it can be expressed as $c_1x_1+c_2x_2+\cdots+ c_n x_n=b$.
    }

    \newd[System of Equations]{d10}{
        Let $\F$ be a field, and $a_{ij}\in\F$ (where $i\in\{1,\ldots,m\}$ and $j\in\{1,\ldots, n\}$). A \tbf{system of linear equations} in variables $x_1,x_2,\ldots,x_n$ is a finite collection of linear equations in $x_1,x_2,\ldots,x_n$: 
        
        \[\begin{aligned}
            a_{11}x_{1}+a_{12}x_{2}+\cdots+a_{1n}x_{n}& =b_{1}  \\
            a_{21}x_{1}+a_{22}x_{2}+\cdots+a_{2n}x_{n}& =b_{2}  \\
            \vdots&\\\
            a_{m1}x_{1}+a_{m2}x_{2}+\cdots+a_{mn}x_{n}& =b_{m}.     
        \end{aligned}\] 
        
        A system of $m$ equations with $n$ unkowns is called an \tbf{$m\times n$ system}.
    }

    \newd[Solutions]{d11}{
        A \tbf{solution} to a linear equation $c_1x_1+c_2x_2+\cdots + c_n x_n=b$ is a choice of field elements $s_1,s_2,\ldots,s_n\in\F$, so that when we substitute them for $x_1,x_2,\ldots,x_n$ respectively, the resulting equation is true.

        That is, we have $c_1s_1+c_2s_2+\cdots+c_n s_n=b$ (i.e. the left- and right-hand sides are equal.)

        A \tbf{solution to a system} is a choice of field elements $s_1,s_2,\ldots,s_n$ which solves $\textit{every}$ equation of the system.
    }

    \newd[Consistent]{d12}{
        If a system of equations has at least one solution, we say it is \tbf{consistent}.

        If a system of equations has no solutions, we say it is \tbf{inconsistent}.
    }
    \envbreak

    \newd[Matrix]{d13}{
        An \tbf{$m\times n$ matrix} over $\F$ is a rectangular array of field elements consisting of $m$ rows and $n$ columns.

        We denote the $j^{\T{th}}$ entry in row $i$ of matrix $A$, by \tbf{$a_{ij}$}, and call it the \tbf{$ij^{\T{th}}$ entry} of $A$.
    }

    \newd[Augmented Matrix]{d14}{
        Consider a system of equations: \[
            \begin{aligned}
                a_{11}x_{1}+a_{12}x_{2}&+\cdots+a_{1n}x_{n}& =b_{1}  \\
                a_{21}x_{1}+a_{22}x_{2}&+\cdots+a_{2n}x_{n}& =b_{2}  \\
                &\vdots &\vdots\\
                a_{m1}x_{1}+a_{m2}x_{2}&+\cdots+a_{mn}x_{n}& =b_{m} 
            \end{aligned}  
        \]

        We defie the \tbf{augmented matrix} \textit{corresponding to} the system of equations above to be: \[
            \left.\left(\begin{array}{cccc|c}a_{11}&a_{12}&\cdots&a_{1n}&b_{1}\\a_{21}&a_{22}&\cdots&a_{2n}&b_{2}\\&&\cdots&&\\a_{m1}&a_{m2}&\cdots&a_{mn}&b_{m}\end{array}\right.\right),
            \]
    }
    \newd[RREF]{d15}{
        We say a matrix $A$ is in \underline{\textbf{reduced row echelon form}} if \textit{all} of the following conditions are met:
        \begin{enumerate}
            \item All zero rows are at the bottom of the matrix $A$.
            \item The first non-zero entry in each non-zero row is a $1$. (Such entries are called ``leading 1's''.)
            \item The leading 1's move to the right, as we go down the rows of $A$.
            \item All entries above and below a leading 1 are 0.
        \end{enumerate}

        We will use the abbreviation "RREF" for "row-reduced echelon form", for the rest of the text.

        \newn{
            All matrices have a unique RREF.
        }
    }

    \newt[Gaussian Elimination]{thm11}{
        To "row reduce" a matrix perform the following steps:
        \begin{enumerate}
            \item If the matrix consists entire of 0's, stop. It's already row-reduced.
            \item Find the first column with a non-zero entry and move the corresponding row to the top. (We will call the first non-zero entry $a$.)
            \item Divide the row by the number $a$ to obtain a leading one.
            \item Subtract multiples of ths row from the rows above and below, in order to make each entry above and below the leading 1 equal to 0.
            \item Repeat 1-4 on the matrix consisting of the remaining rows.
        \end{enumerate}
    }

    \newd[Variables]{def16}{
        Suppose that $R$ is a matrix in RREF. We say that $x_i$ is a \underline{\tbf{leading variable}} if column $i$ contains a leading one. If a variable is not "leading" we call it a \underline{\tbf{non-leading variable}}.
    }
    \newr{
        To solve a system:
        \begin{enumerate}
            \item Row reduce the augmented coefficient matrix.
            \item If there is a row of the form $(\quad0\quad0\quad\cdots\quad0\quad|\quad1\quad)$ then there are no solutions.
            \item Otherwise, assign the non-leading variables (if any) parameters, and use the equatoins coming from the rows of the RREF to solve for each variable interms of the parameters.
        \end{enumerate}
    }

    \newd[Homogeneous]{d17}{
        A system of equations is called \underline{\tbf{homogeneous}} if it is of the form \[\begin{aligned}
            a_{11}x_{1}+a_{12}x_{2}+\cdots+a_{1n}x_{n}& =0  \\
            a_{21}x_{1}+a_{22}x_{2}+\cdots+a_{2n}x_{n}& =0  \\
            &\vdots \\
            a_{m1}x_{1}+a_{m2}x_{2}+\cdots+a_{mn}x_{n}& =0. 
            \end{aligned}\]
        In other words, it is homogeneous if the constant term (or right hand side) of \tbf{\textit{each}} equation in the system is 0.
        \newn{
            \begin{itemize}
                \item $x_1=0,x_2=0,\ldots,x_n=0$ is \tbf{\textit{always}} a solution to any homogeneous equation.
                \item We call this solution the \tbf{\textit{trivial}} solution.
                \item Any other solution is called a \tbf{\textit{non-trivial}} solution.
            \end{itemize}
        }
    }
}

\newpage
\newch[Vector Spaces]{ch3}{
    \newd[Vector Operators]{d18}{
        Given two vectors \tbf{v,w} we define \underline{\tbf{their sum}} \tbf{v+w} using “tip to tail" addition (or the “parallelogram law of addition"). In the diagram in the margin, the vector \tbf{v+w} is diagonal in the parallelogram spanned by v, w that shares its tail with v and w.

        We can also define their \tbf{\underline{difference}} \tbf{v-w} geometrically using the same parallelogram: v- w is the diagonal going from the tip of w to the tip of v.

        Finally, given a vector v and real number $a\in\mathbb{R}$, we can \underline{\tbf{scale v by $a$}} as follows:
        
        \begin{itemize}
            \item $0\mathbf{v}=\mathbf{0}.$
            \item If $a>0$, then $a\mathbf{v}$ is a vector pointing in the same direction as v with length scaled by $a$
            \item If $\alpha<0$, then $\alpha$vis a vector pointing in the opposite direction as v with length scaled by $|\alpha|.$
        \end{itemize}

        \newn{
            If $v=(x,y,z)$ and $w=(p,q,r)$, then $v+w=(x+p,y+q,z+r)$, $av=(ax,ay,az)$.
        }
    }

    \tcbcnt{definition}{19}

    \newd[$\F^n$]{def20}{
        Let $\F$ be a field. Consider the set $\F^n=\{(x_1,x_2,\ldots,x_n)\mid x_1,x_2,\ldots,x_n\in\F\}$. We can define two operations on $\F^n$ which we call "vector addition" which is a map $\F^n\times\F^n\ra\F^n$, and "scaling" which is a map $\F\times\F^n\ra\F^n$ as follows.

        For $v=(x_1,x_2,\ldots,x_n), w=(y_1,y_2,\ldots,y_n)\in\F^n$, and $c\in\F$ we define:
        \begin{align*}
            \Al{v+w}{(x_1,x_2,\ldots, x_n) + (y_1,y_2,\ldots,y_n)}{(vector addition)}
            &=(x_1+y_1,x_2+y_2,\ldots,x_n+y_n) &\\
            \Al{cv}{c(x_1,x_2,\ldots,x_n)}{(scaling)}
            &=(cx_1,cx_2,\ldots,cx_n) &
        \end{align*}
    }

    \tcbcnt{theorem}{12}
    \newt[$\F^n$]{thm13}{
        Let $\F$ be a field. Set $0=(0,0,\ldots,0)$. For any $v,w,u\in\F^n$ and $a,b\in\F$ we have:
        \begin{enumerate}
            \item $v+w=w+v$.
            \item $v+(w+u)=(v+w)+u$.
            \item $a(v+w)=av+aw$.
            \item $(a+b)v=av+bv$.
            \item $(ab)v=a(bv).$
            \item $1v=v$.
            \item $0+v=v$.
            \item For every $v\in V$ there exists $w\in V$ so that $v+w=0$.
        \end{enumerate} 
    }

    \envbreak
    \newd[Vector Space]{def21}{
        Let $\F$ be a field. A vector space $V$ over $\F$ is a non-empty set, containing a special element 0, with two operatoins $V\times V\ra V$ (called vector addition) and $\F\times V\ra V$ (called scaling) so that for all $v,w,u\in V$ and $a,b\in\F$:
        \begin{enumerate}
            \item $v+w=w+v$.
            \item $v+(w+u)=(v+w)+u$.
            \item $a(v+w)=av+aw$.
            \item $(a+b)v=av+bv$.
            \item $(ab)v=a(bv).$
            \item $1v=v$.
            \item $0+v=v$.
            \item For every $v\in V$ there exists $w\in V$ so that $v+w=0$.
        \end{enumerate}
        \newn{
            $P(\F), P_n(\F)$ and $\F^n$ are vector spaces.
        }
    }

    \envbreak
    \newd[Matrix Cont.]{def22}{
        Let $\F$ be a field. An $m\times n $ \tbf{matrix $M$ with entries in $\F$} is a rectangular array of elements of $\F$ consisting of $m$ rows and $n$ columns.

        We denote the entry in the $i$ row and $j$ column of a matrix $m$ by $m_{ij}$.

        The set of all $m\times n$ matrices with coefficients in $\F$ is denoted by $\mathcal{M}_{m\times n}(\F)$.

        For example, a $2\times3$ matrix looks like $\begin{pmatrix}m_{11}&m_{12}&m_{13}\\m_{21}&m_{22}&m_{23}\end{pmatrix}\in\mathcal{M}_{2\times3}(\F)$, wile a $3\times 2$ matrix $N$ looks like $\begin{pmatrix}n_{11}&n_{12}\\n_{21}&n_{22}\\n_{31}&n_{32}\end{pmatrix}\in\mathcal{M}_{3\times2}(\F)$.

        \newn{
            $\mathcal{M}_{m\times n}(\F)$, with pointwise addition and scaling is a vector space over $\F$;
        }
    }

    \newco[Vector Space]{co2}{
        Let $\F$ be a field, and $V$ a vector space over $\F$. THen for any $v,w,u\in V$ and $a\in\F$ we have:
        \begin{enumerate}
            \item If $v+w=v+u$, then $w=u$.
            \item If $a\neq 0$ and $av=aw$, then $v=w$.
            \item The element $0\in V$ is unique.
            \item Additive inverses in $V$ are unique. (This means that for each $v\in V$ there is only one element $w\in V$ which satisfies the condition of Axiom 8.)
            \item $(-a)v=-(av)$. In particupar $(-1)v=-v$.
            \item $0v=0$.
            \item $a0=0$.
        \end{enumerate}
    }

    \newd[Subspace]{def23}{
        Let $\F$ be a field and $V$ a vector space over $\F$. We say that a subset $W\subseteq V$ is \tbf{a subspace of} $V$ if $W$ is also a vector space over $|F$ using the same operations defined in $V$.

        \newn{
            $P_n(\F)$ is a subspace of $P(\F)$.
            
            $P_n(\F)$ is a subspace of $P_m(\F)$ if $n<m\in\N$. 
        }
    }

    \tcbcnt{theorem}{18}
    \newt[Subspace]{thm19}{
        Let $V$ be a vector space over a field $\F$. A \tbf{non-empty} subset $W\subseteq V$ is a subspace of $V$ if and only if 

        \begin{enumerate}
            \item For all $v,w\in W$ we have $v+w\in W$.
            \item For all $v\in W$ and $c\in \F$ we have $cv\in W.$
        \end{enumerate}
    }

    \newd[Trivial / Non-Trivial Subspace]{def24}{
        Let $V$ be a vector space over a field $\F$. The subspaces $\{0\}$ and $V$ are called the \tbf{trivial subspaces} of $V$. Any other subspace $W$ of $V$ is called a \tbf{non-trivial subspace of} $V$.

        In particular, we say that a subspace $W$ is a non-trivial subspace of $V$ if $W\neq\{0\}$ and $W\neq V$.
    }
}

\newpage
\newch[Bases and Dimension]{ch4}{
    \newd[Linear Combinatoin of Vectors]{def25}{
        Let $V$ be a vector space over $\F$, and $v_1,v_2,\ldots,v_k\in V$. A vector of the form $a_1 v_1 + a_2 v_2 +\cdots + a_k v_k \in V$ is called a linear combination of the vectors $v_1,v_2,\ldots, v_k$.
    }

    \newd[Span]{def26}{
        Let $V$ be a vector space over $\F$ and $S\subseteq V$. We define the \tbf{span of} $S$, denoted $\vspan S$, as follows:
        \begin{enumerate}
            \item If $S=\varnothing$ is empty, then $\vspan S=\{0\}$.
            \item Otherwise, $\vspan S=\{a_1 v_1 + a_2 v_2 +\cdots + a_k v_k | a_i\in\F v_i \in S\}$ is the set of all possible linear combinations of vectors from $S$.
        \end{enumerate}
    }

    \newt[Span as Subspace]{thm20}{
        Let $V$ be a vector space over $\F$ and $S\subseteq V$ be \tbf{any} subset of vectors. Then the subset $\vspan S\subseteq V$ is a subspace of $V$.
    }

    \newd[$S$ spans $V$]{def27}{
        Let $V$ be a vector space over $\F$. We say that a subset $S\subseteq V$ is \tbf{a spanning set for} $V$ (or "$S$ \tbf{spans} $V$") if $V=\vspan S$.
    }

    \envbreak
    \newd[Linearly Independnet]{def28}{
        Let $V$ be a vector space over $\F$. We say that a set $S$ is \tbf{linearly independent} if for any vectors $v_1,v_2,\ldots,v_k\in S$: \[c_1v_1+c_2v_2+\cdots+c_kv_k=0\implies c_1=0,c_2=0,\ldots,c_k=0.\] Otherwise, we say that $S$ is \tbf{linearly dependent}.
    }
    \newd[Basis]{def29}{
        Let $V$ be a vector space over $\F$. A subset $\beta\subseteq V$ is called a \tbf{basis} if:
        \begin{enumerate}
            \item $\beta$ spans $V$
            \item $\beta$ is linearly independent.
        \end{enumerate}
        \newn{
            Vector space over $\F$ also has basis.

            Finite spanning set for $V$ also contains basis for $V$.
        }
        \newm{
            Let $\F$ be a field.
            \begin{enumerate}
                \item The set $\{e_1,e_2,\ldots,e_n\}$ is a basis for $\F^n$.
                \item The set $\{E_{ij}|1\leq i\leq m, 1\leq n\}$ is a basis for $\mathcal{M}_{m\times n}(\F)$.
                \item The set $\{1,x,x^2,x^3,\ldots\}$ is a basis for $P(\F)$.
                \item The set $\{1,x,x^2,x^3,\ldots,x^n\}$ is a basis for $P_n(\F)$.
            \end{enumerate}
        }
    }
    \tcbcnt{theorem}{22}
    \newt[Unique Expression from Basis]{thm23}{
        Let $V$ be a vector space over $\mathbb{F}$ and $\beta$ a basis of $V$. Then any $\mathbf{v} \in V$ has a unique expression
        $$
        \mathbf{v}=\sum_{i=1}^n a_i \mathbf{v}_i
        $$
        where $\mathbf{v}_i \in \beta$ and $a_i \in \mathbb{F}$.
    }
    \newt[The Replacement Theorem]{thm24}{
        Suppose that $\beta=\{\mathbf{v}_1,\mathbf{v}_2,...,\mathbf{v}_n\}$ is a basis for $V$ and $I=\left\{\mathbf{w}_1,\mathbf{w}_2,\ldots,\mathbf{w}_k\right\}$ an independent subset of $V.$ Then for any $i\in \{ 1, ..., k\} $, we can obtain a new basis by replacing $i$ elements of $\beta$ with
        $\{\mathbf{w}_1,\ldots,\mathbf{w}_i\}.$
        So after relabelling the elements* $\mathbf{v}_j\in\beta$ we have that the set $\beta_i=\{\mathbf{w}_1,\ldots,\mathbf{w}_i,\mathbf{v}_{i+1},\ldots\mathbf{v}_n\}$ is a basis
        for $V.$

        \newco{co3}{
            Suppose that $V$ is a vector space over $\F$ with a finite basis. Suppose that $\beta$ is any basis of $V$ and $I$ is any independent set. Then $|I|\leq |B|$.
        }

    }
    \newt[Size of Bases]{thm25}{
        Let $V$ be a vector space over a field $\F$. If $V$ has a finite basis, then all bases of $V$ have the same size.
    }
    \newd[Dimension]{def30}{
        Let $V$ be a vector space over $\F$ with a finite basis. We define the \tbf{dimension of} $V$ to be the size of a basis for $V$.

        In this case, we say that $V$ \tbf{is finite dimensional}. Otherwise, we say that $V$ is infinite dimenisonal.
        \newn{
            \begin{enumerate}
                \item $\dim \F^n=n$.
                \item $\dim \mathscr{M}_{m\times n}(\F)=mn.$
                \item $\dim P_n(\F)=n+1.$
                \item $P(\F)$ is infinite dimensional.
            \end{enumerate}


        }
    }
    \newco{co4}{
        Let $V$ be a finite dimensional vector space over $\F$. $S$ any spanning set for $V$, $I$ any independent set in $V$, and $\beta$ any basis. Then \[|I|\leq |\beta|\leq |S|.\]
    }

}

\newpage

\newch[Linear Transformations]{ch5}{
    \newd[Linear Transformation]{d31}{
        Let $V$ and $W$ be vector spaces over $\F$. A map $T:V\ra W$ is called a \tbf{linear transformation} if:
        \begin{align*}
            \Al{T(v+w)}{T(v)+T(w)}{for all $v,w\in V$}
            \Al{T(cv)}{cT(v)}{for all $v\in V$ and $c\in\F$}
        \end{align*}
        \newn{
            $T(x,y,z)=(2x-4y+z,3x-y+2x)$ is a linear transformation from $\R^3$ to $\R^2$.

            $T(p)=\frac{\D}{\D x}p$ is a linear transformation.
        }
    }

    \newt[Properties of Linearity]{thm26}{
        Let $V,W$ be vector spaces over $\F$.
        \begin{enumerate}
            \item If $T:V\ra W$ is linear, then $T(0_v)=0_w$.
            \item The map $O: V\ra W$ given by $O(v)=0_w$ for all $v\in V$ is linear. This map is called the "zero map."
            \item The map $I_V:V\ra V$ given by $I_V(v)=v$ for all $v\in V$ is linear. This map is called the "identity map."
        \end{enumerate}
    }

    \newt{thm27}{
        Let $V$ e a finite dimensional vector space over $\F$ and $\beta=\{v_1,\ldots,v_n\}$ a basis of $V$. A linear map $T: V\ra W$ is uniquely determined by the values $T(v_1),T(v_2),\ldots,T(v_n)\in W$.
    }

    \newco[Extending by Linearity]{co5}{
        Let $V,W$ be vector spaces over $\F$, and $\beta=\{v_1,\ldots,v_n\}$ a basis for $V$. Given a list of (not necessarily distinct) vectors $w_1,\ldots, w_n\in W$ there is exactly one linear map $T: V\ra W$ so that $W(v_i)=w_i$.

        This map is defined for all $v\in V$ as follows. Writing $v=\displaystyle\sum_{i=1}^n a_i v_i$, we then set $T(v)=\displaystyle\sum_{i=1}^n a_i w_i$.

        This process is called "extending by linearity".
    }

    \newt[Composition of Linear Maps]{thm28}{
        Let $V,W,X$ be vector spaces over $\F$. If $T:V\ra W$ and $S:W\ra X$ are linear maps, then the composition $S\circ T:V\ra X$ is linear.
    }

    \envbreak

    \newt[Null Space / Image]{thm29}{
        Let $V,W$ be vector spaces over $\F$ and $T:V\ra W$ a linear transformation. The sets: 
        \[N(T)=\{v\in V\mid T(v)=0\}\subseteq V\]
        \[\operatorname{im}(T)=\{w\in W\mid w=T(v)\text{ for some }v\in V\}\subseteq W\]

        are subspaces of $V,W$ respectively.

        The subspace $N(T)$ is called the \tbf{null space of} $T$.

        The subspace $\operatorname{im}(T)$ is called the \tbf{image of} $T$.
    }

    \newd[rank]{def32}{
        Let $V,W$ be vector spaces over $\F$ and $T:V\ra W$ linear. We define the \tbf{rank} of $T$ by $\operatorname{rank} T=\dim \operatorname{im}(T)$.
    }

    \newt[The Dimension Theorem]{thm30}{
        Let $V,W$ be finite dimenisonal vector spaces over $\F$. If $T:V\ra W$ linear, then $$\dim V=\dim N(T)+\dim \operatorname{im}(T)$$.
    }

    \newd[$\mathcal{L}(V,W)$]{def33}{
        Let $V,W$ be vector spaces over a field $\F$. We denote by $\mathcal{L}(V,W)=\{T:V\to W\mid T \T{ is linear}\}$ the set of all linear transformations from $V$ to $W$.
        
        We define addition and scaling of transformations by:

        \begin{align*}
            \Al{(T+S)(v)}{T(v)+S(v)}{(we can add $T(v),S(v)\in W$)}
            \Al{(cT)(v)}{cT(v)}{(we can scale the vector $T(v)\in W$ by $c\in\F$)}
        \end{align*}
    }

    \newt{thm31}{
        Let $V,W$ be vector spaces over a field $\F$. If $S,T\in\mathcal{L}(V,W),$ and $c\in\F$ then $T+S, cT\in\mathcal{L}(V,W)$.
    }

    \newt{thm32}{
        Let $V,W$ be vector spaces over a field $\F$. \begin{enumerate}
            \item The set $\mathcal{L}(V,W)$ is a vector space over $\F$.
            \item If $\dim V=n,\dim W=m,$ then $\dim \mathcal{L}(V,W)=nm$. 
        \end{enumerate}
    }

    \newd[Invertibility]{def34}{
        Let $A,B$ be sets and $f: A\to B$ be a function.
        \begin{enumerate}
            \item We say that $f$ is injective if for all $x,y\in A, f(x)=f(y)\implies x=y.$
            \item We say that $f$ is surjective if $\im f = f(A) = B$.
            \item We say that $f$ is bijective if it is both injective and surjective.
            \item We say that $f$ is \tbf{invertible}, if there exists a function $g: W\to V$ so that $g\circ f=I_A:A\to A$ and $f\circ g=I_B:B\to B$.

            In this case we call the map $g$ the \tbf{inverse} of $f$ and denote it by $f^{-1}$.
        \end{enumerate}
    }

    \newt{thm33}{
        Let $V,W$ be vector spaces over $\F$. If $T: V\to W$ is linear and bijective, then the inverse $T^{-1}: W\to V$ is also linear.
    }

    \newd[Isomorphism]{def35}{
        Let $V,W$ be vector spaces over the field $\F$. We say that a linear map $T: V\to W$ is an \tbf{isomorphism} if it is bijective.

        We say that $V$ is \tbf{isomorphic to} $W$, and write $V\simeq W$, if there exists an isomorphism $T:V\to W$.
    }

    \newt{thm34}{
        Let $V,W$ be vector spaces over $\F$ and $T: V\to W$ be linear. Then $T$ is injective if and only if $N(T)+\{0_v\}$.
    }

    \newt{thm35}{
        Let $V,W$ be finite dimensional vector spaces over $\F$. Then $V\simeq W$ if and only if $\dim V=\dim W$.
    }

    \newt{thm36}{
        Let $V,W$ be finite dimensional vector spaces over the field $\F$.

        If $T: V\to W$ is linear and $\dim V=\dim W$, then the following are equivalent:

        \begin{enumerate}
            \item $T$ is injective.
            \item $T$ is surjective.
            \item $T$ is an isomorphism.
        \end{enumerate}
    }

    \newt{thm37}{
        The maps $R_\theta, \T{proj}_L,\R_L:\R^2\ra\R^2$ are linear.
    }

    \newt{thm38}{
        The maps $R_{\theta, L}, \T{proj}_L, R_P:\R^3\to\R^3$ are linear.
    }

}

\newpage
\newch[Coordinates]{ch6}{
    \newt{thm39}{
        Let $A\in\mathscr{M}_{m\times n}(\F)$. Define $T_A:\F^n\to \F^m$ by $T_A(x)=Ax$.
        \begin{enumerate}
            \item The map $T_A$ is linear.
            \item The map $F:\mathscr{M}_{m\times n}(\F)\to\mathcal{L}(\F^n,\F^m)$ given by $F(A)=T_A$ is an isomorphism of vector spaces. That is, every linear map $T:\F^n\to \F^m$ is given by matrix multiplication for some matrix $A$.
        \end{enumerate}
    }

    \newd{def36}{
        Let $V$ be a finite dimensional vector space over $\F$. An \tbf{ordered basis} for $V$ is a basis $\beta$, together with a \tbf{fixed} order for listing its elements: $\beta=\{v_1,\ldots,v_n\}$.
    }

    \newt{thm40}{
        Let $V$ be a vctor space over $\F$ of dimension $n$, and $\beta=\{v_1,\ldots,v_n\}$ a basis of $V$. The map $\phi_{\beta}:V\to \F^n$ defined above is an isomorphism of vector spaces.

        We will denote by $[v]_\beta$ the element $\phi_\beta(v)$. I.e. $\phi_\beta(v)=[v]_\beta$.

        We will call a choice of basis on $V$, together with the isomorphism $\phi_beta:V\to \F^n$ a ``\tbf{coordinate system}'' on $V$.

    }

    \newd{def37}{
        Let $V,W$ be finite dimensional vector spaces over $\F$. Let $\beta=\{v_1,\ldots,v_n\}$ be a basis of $V$ and $\gamma=\{w_1,\ldots,w_m\}$ a basis of $W$. For $T\in\mathcal{L}(V,W)$ we define the matrix $[T]_\beta^\gamma\in\mathscr{L}(\F^n,\F^m)$ as follows:
        \begin{enumerate}
            \item The columns of $[T]_\beta^\gamma$ are given by: $[T(v_1)]_\gamma,\ldots,[T(v_n)]_\gamma$.Alternatively: we can express $T(v_j)\in W$ using the basis $\gamma$ to obtain an expression $T(v_j)=\ds\sum_{i=1}^m A_{ij}w_i$. We then define $\bra{[T]_\beta^\gamma}_{ij}=A_{ij}$.
        \end{enumerate}

        When $T: V\to V$ we denote by $[T]_\beta=[T]_beta^\beta$.
    }

    \newt{thm41}{
        Let $V,W$ be finite dimensional vector spaces over $\F$, $\beta=\{v_1,\ldots,v_n\}$ a basis of $V$, and $\gamma=\{w_1,\ldots,w_m\}$ a basis of $W$. 

        The map $\phi_\beta^\gamma: \mathcal{L}(V,W)\to\mathscr{M}_{m\times n}(\F)$ given by $\phi_\beta^\gamma(T)=[T]_\beta^\gamma$ is an isomorphism of vector spaces.
    }

    \newt{thm42}{
        Let $V,W,X$ be finite dimensional vector spaces over $\F$. Let $\beta=\{v_1,\ldots,v_n\}$ be a basis of $V$, $\gamma=\{w_1,\ldots,w_m\}$ a basis of $W$, and $\delta=\{x_1,\ldots,x_p\}$ a basis of $X$. 

        For all $T\in\mathscr{L}(V,W)$ and $S\in\mathscr{L}(W,X)$ we have that \[[S\circ T]_\beta^\delta=[S]_gamma^\delta[T]_\beta^\gamma.\] In other words, when using compatible coordinate systems, the composition of linear transformations corresponds to matrix multiplication.
    }

    \newt{thm43}{
        Let $V,W$ be finite dimensional vector spaces over $\F$, $T: V\to W$ be a linear map, $\beta=\{v_1,\ldots,v_n\}$ a basis of $V$, and $\gamma=\{w_1,\ldots,w_m\}$ a basis of $W$. Then $T$ is invertible if and only if $[T]_\beta^\gamma$ is invertible.

        Moreover, we have \[[T^{-1}]_\gamma^\beta=\bra{[T]_\beta^\gamma}^{-1}\] if $T$ is invertible.
    }

    \newt{thm44}{
        Let $V,W$ be finite dimensional vector spaces over a field $\F, \beta=\{v_1,\ldots,v_n\}$ a basis of $V$, and $\gamma=\{w_1,\ldots,w_m\}$ a basis of $W$. Then we have \[[T]_\beta^\gamma[v]_\beta=[T(v)]_\gamma\] for all $v\in V$.
    }
}

\newpage

\newch[Matrix Algebra (Appendix A)]{ch6}{
    \newd[Matrix Multiplication]{def86}{
        Let $A\in\mathscr{M}_{m\times n}(\F)$ and $B\in\mathscr{M}_{n\times k}(\F)$. We define their product $AB\in\mathscr{M}_{m\times k}(\F)$ as follows: for $i\in\{1,\ldots,m\}$ and $j\in\{1,\ldots,k\}$ the $ij$-entry of the product $AB$ is given by 
        
        \[(AB)_{ij}=\sum_{l=1}^n A_{il}B_{lj}.\]

        \newn{
            $AB\neq BA$.
        }
    }

    \newd[Special Matrices]{def87}{
        For each $n,m\in\N$ we define the following matrices:
        \begin{enumerate}
            \item $O_{m,n}\in\mathscr{M}_{m\times n}(\F)$ - the matrix consisting of all 0's. In other words $(O_{m,n})_{i,j}=0$ for all $i\in\{1,\ldots,m\}$ and $j\in\{1,\ldots,n\}$. 
            \item $I_n\in\mathscr{M}_{n\times n}(\F)$ - the matrix with 1's on the diagonals, and 0 in all other entries. In other words 
            
            $$(I_n)_{ij}=\begin{cases}1&\text{if }i=j\\0&\text{if }i\neq j\end{cases}$$\\
        \end{enumerate}

        \newm{
            $$O_{2,3}= \begin{pmatrix}0&0&0\\0&0&0\\\end{pmatrix}$$
        
            $$I_3=\begin{pmatrix}1&0&0\\0&1&0\\0&0&1\\\end{pmatrix}$$
        }
    }

    

    \newt{thm124}{
        Let $\mathbb{F}$ be a field, $A, A_1, A_2 \in \mathscr{M}_{\mathbf{m} \times \mathbf{n}}(\mathbb{F}), B, B_1, B_2 \in \mathcal{M}_{n \times k}(\mathbb{F}), C \in \mathcal{M}_{k \times p}(\mathbb{F})$ and $c \in \mathbb{F}$.
        
        1. $A(B C)=(A B) C$
        
        2. $\left(A_1+A_2\right) B=A_1 B+A_2 B$
        
        3. $A\left(B_1+B_2\right)=A B_1+A B_2$
        
        4. $I_m A=A=A I_n$
        
        5. $\mathrm{O}_{r m} A=\mathrm{O}_{r n}$ for any 
        $r \in \mathbb{N}$.
        
        6. $A(c B)=c(A B)=\left(c \mathrm{I}_m\right) A B=A B\left(c \mathrm{I}_k\right)=A\left(c \mathrm{I}_n\right) B$.
    }

    \newd[Invertibility]{def88}{
        Let $A\in\mathscr{M}_{n\times n}(\F)$. We say that $A$ is \tbf{invertible} if there exists a matrix $B\in\mathscr{M}_{n\times n}(\F)$ so that $AB=I_n=BA$. 
    }

    \newt{thm125}{
        Let $A,B\in\mathscr{M}_{n\times n}(\F)$. 
        \begin{enumerate}
            \item If $A$ is invertible, then the inverse of $A$ is unique.
            \item If $A$ is invertible, then $A^{-1}$ is also invertible.
            \item If $A$ and $B$ are invertible, then $AB$ is invertible.
            \item $I_n$ is invertible.
            \item If $AB=I_n$, then $A$ is invertible and $B=A^{-1}$.
        \end{enumerate}
    }

    \newd[$A^t$]{def89}{
        Let $A\in\mathscr{M}_{m\times n}(\F)$. We define the matrix $A^t\in\mathscr{M}_{n\times m}(\F)$  by:

        \[(A^t)_{ij}=A_{ji}.\]

        In other words, to obtain $A^t$ we "swap the rows and columns of A."
    }

    \newd[Symmetric]{def90}{
        We say that $A\in\mathscr{M}_{n\times n}(\F)$ is \tbf{symmetric} if $A^t=A$. We denot ethe set of all symmetric matrices by \tbf{Sym}$_n(\F)$.

        We say that $A\in\mathscr{M}_{n\times n}(\F)$ is \tbf{skew-symmetric} if $A^t=-A$. We denote the set of all skew-symmetric matrices by \tbf{Sk}$_n(\F)$.
    }

    \newt{thm126}{
        Let $A, B \in \mathscr{M}_{\mathbf{m} \times \mathbf{n}}(\mathbb{F}), C \in \mathscr{M}_{n \times k}(\mathbb{F})$ and $c \in \mathbb{F}$.
        
        1. $(A+B)^t=A^t+B^t$
        
        2. $(c A)^t=c A^t$
        
        3. $\left(A^t\right)^t=A$
        
        4. $(A C)^t=C^t A^t$.
        
        5. In the case that $m=n$, we also have that if $A \in \mathscr{M}_{\mathbf{n} \times \mathbf{n}}(\mathbb{F})$ is invertible, then $A^t \in \mathscr{M}_{\mathbf{n} \times \mathbf{n}}(\mathbb{F})$ is invertible and $\left(A^t\right)^{-1}=\left(A^{-1}\right)^t$.
    }

    \newd[Diagonal and Triangular]{def91}{
        We say that $A$ is \tbf{diagonal} if $A_{ij}=0$ for all $i\neq j$. 

        Let $A\in\mathscr{M}_{n\times n}(\F)$. We say that $A$ is \tbf{upper triangular} if $A_{ij}=0$ for all $i>j$. This means that all entries below the diagonal of $A$ must be 0.

        Similarily, we say that $A$ is \tbf{lower triangular} if $A_{ij}=0$ for all $i<j$. This means that all entries above the diagonal of $A$ must be 0.

        We say that $A$ is \tbf{strictly upper-triangular} if $A_{ij}=0$ for all $i\geq j$. 
    }
}

\end{document}